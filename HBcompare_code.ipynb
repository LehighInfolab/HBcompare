{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OOeyzzhd6mVa"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!pwd\n",
        "os.chdir('gdrive/My Drive/Graph_based_methods/HBcompare/')\n",
        "!pwd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "remLVL7udJ6n"
      },
      "source": [
        "### Util.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PgqBNOvTeA6T"
      },
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "import numpy as np\n",
        "import random\n",
        "import scipy.sparse as sp\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from scipy.sparse import isspmatrix\n",
        "\n",
        "\"\"\"Adapted from https://github.com/weihua916/powerful-gnns/blob/master/util.py\"\"\"\n",
        "\n",
        "class S2VGraph(object):\n",
        "    def __init__(self, g, label, node_tags=None, node_features=None):\n",
        "        '''\n",
        "            g: a networkx graph\n",
        "            label: an integer graph label\n",
        "            node_tags: a list of integer node tags\n",
        "            node_features: a torch float tensor, one-hot representation of the tag that is used as input to neural nets\n",
        "            edge_mat: a torch long tensor, contain edge list, will be used to create torch sparse tensor\n",
        "            neighbors: list of neighbors (without self-loop)\n",
        "        '''\n",
        "        self.label = label\n",
        "        self.g = g\n",
        "        self.node_tags = node_tags\n",
        "        self.neighbors = []\n",
        "        self.node_features = 0\n",
        "        self.edge_mat = 0\n",
        "        self.max_neighbor = 0\n",
        "\n",
        "\n",
        "def load_data(dataset, degree_as_tag):\n",
        "    '''\n",
        "        dataset: name of dataset\n",
        "        test_proportion: ratio of test train split\n",
        "        seed: random seed for random splitting of dataset\n",
        "    '''\n",
        "\n",
        "    print('loading data')\n",
        "    g_list = []\n",
        "    label_dict = {}\n",
        "    feat_dict = {}\n",
        "\n",
        "    with open('/content/gdrive/My Drive/Graph_based_methods/Graph_Transformer/dataset/%s/%s.txt' % (dataset, dataset), 'r') as f:\n",
        "        n_g = int(f.readline().strip())\n",
        "        for i in range(n_g):\n",
        "            row = f.readline().strip().split()\n",
        "            n, l = [int(w) for w in row]\n",
        "            if not l in label_dict:\n",
        "                mapped = len(label_dict)\n",
        "                label_dict[l] = mapped\n",
        "            g = nx.Graph()\n",
        "            node_tags = []\n",
        "            node_features = []\n",
        "            n_edges = 0\n",
        "            for j in range(n):\n",
        "                g.add_node(j)\n",
        "                row = f.readline().strip().split()\n",
        "                tmp = int(row[1]) + 2\n",
        "                if tmp == len(row):\n",
        "                    # no node attributes\n",
        "                    row = [int(w) for w in row]\n",
        "                    attr = None\n",
        "                else:\n",
        "                    row, attr = [int(w) for w in row[:tmp]], np.array([float(w) for w in row[tmp:]])\n",
        "                if not row[0] in feat_dict:\n",
        "                    mapped = len(feat_dict)\n",
        "                    feat_dict[row[0]] = mapped\n",
        "                node_tags.append(feat_dict[row[0]])\n",
        "\n",
        "                if tmp > len(row):\n",
        "                    node_features.append(attr)\n",
        "\n",
        "                n_edges += row[1]\n",
        "                for k in range(2, len(row)):\n",
        "                    g.add_edge(j, row[k])\n",
        "\n",
        "            if node_features != []:\n",
        "                node_features = np.stack(node_features)\n",
        "                node_feature_flag = True\n",
        "            else:\n",
        "                node_features = None\n",
        "                node_feature_flag = False\n",
        "\n",
        "            assert len(g) == n\n",
        "\n",
        "            g_list.append(S2VGraph(g, l, node_tags))\n",
        "     \n",
        "\n",
        "    #add labels and edge_mat       \n",
        "    for g in g_list:\n",
        "        g.neighbors = [[] for i in range(len(g.g))]\n",
        "        for i, j in g.g.edges():\n",
        "            g.neighbors[i].append(j)\n",
        "            g.neighbors[j].append(i)\n",
        "        degree_list = []\n",
        "        for i in range(len(g.g)):\n",
        "            g.neighbors[i] = g.neighbors[i]\n",
        "            degree_list.append(len(g.neighbors[i]))\n",
        "        g.max_neighbor = max(degree_list)\n",
        "\n",
        "        g.label = label_dict[g.label]\n",
        "\n",
        "        edges = [list(pair) for pair in g.g.edges()]\n",
        "        edges.extend([[i, j] for j, i in edges])\n",
        "\n",
        "        deg_list = list(dict(g.g.degree(range(len(g.g)))).values())\n",
        "\n",
        "        g.edge_mat = np.transpose(np.array(edges, dtype=np.int32), (1,0))\n",
        "\n",
        "    if degree_as_tag:\n",
        "        for g in g_list:\n",
        "            g.node_tags = list(dict(g.g.degree).values())\n",
        "\n",
        "    #Extracting unique tag labels   \n",
        "    tagset = set([])\n",
        "    for g in g_list:\n",
        "        tagset = tagset.union(set(g.node_tags))\n",
        "\n",
        "    tagset = list(tagset)\n",
        "    tag2index = {tagset[i]:i for i in range(len(tagset))}\n",
        "\n",
        "    for g in g_list:\n",
        "        g.node_features = np.zeros((len(g.node_tags), len(tagset)), dtype=np.float32)\n",
        "        g.node_features[range(len(g.node_tags)), [tag2index[tag] for tag in g.node_tags]] = 1\n",
        "\n",
        "\n",
        "    print('# classes: %d' % len(label_dict))\n",
        "    print('# maximum node tag: %d' % len(tagset))\n",
        "\n",
        "    print(\"# data: %d\" % len(g_list))\n",
        "\n",
        "    return g_list, len(label_dict)\n",
        "\n",
        "\n",
        "def separate_data(graph_list, fold_idx, seed=0):\n",
        "    assert 0 <= fold_idx and fold_idx < 10, \"fold_idx must be from 0 to 9.\"\n",
        "    skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
        "\n",
        "    labels = [graph.label for graph in graph_list]\n",
        "    idx_list = []\n",
        "    for idx in skf.split(np.zeros(len(labels)), labels):\n",
        "        idx_list.append(idx)\n",
        "    train_idx, test_idx = idx_list[fold_idx]\n",
        "\n",
        "    train_graph_list = [graph_list[i] for i in train_idx]\n",
        "    test_graph_list = [graph_list[i] for i in test_idx]\n",
        "\n",
        "    return train_graph_list, test_graph_list\n",
        "    \n",
        "\n",
        "\"\"\"Get indexes of train and test sets\"\"\"\n",
        "def separate_data_idx(graph_list, fold_idx, seed=0):\n",
        "    assert 0 <= fold_idx and fold_idx < 10, \"fold_idx must be from 0 to 9.\"\n",
        "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
        "\n",
        "    labels = [graph.label for graph in graph_list]\n",
        "    idx_list = []\n",
        "    for idx in skf.split(np.zeros(len(labels)), labels):\n",
        "        idx_list.append(idx)\n",
        "    train_idx, test_idx = idx_list[fold_idx]\n",
        "\n",
        "    return train_idx, test_idx\n",
        "\n",
        "\"\"\"Convert sparse matrix to tuple representation.\"\"\"\n",
        "def sparse_to_tuple(sparse_mx):\n",
        "    def to_tuple(mx):\n",
        "        if not sp.isspmatrix_coo(mx):\n",
        "            mx = mx.tocoo()\n",
        "        coords = np.vstack((mx.row, mx.col)).transpose()\n",
        "        values = mx.data\n",
        "        shape = mx.shape\n",
        "        return coords, values, shape\n",
        "\n",
        "    if isinstance(sparse_mx, list):\n",
        "        for i in range(len(sparse_mx)):\n",
        "            sparse_mx[i] = to_tuple(sparse_mx[i])\n",
        "    else:\n",
        "        sparse_mx = to_tuple(sparse_mx)\n",
        "\n",
        "    return sparse_mx\n",
        "\n",
        "\n",
        "# data_name = \"dataset1\"\n",
        "# use_degree_as_tag = False\n",
        "\n",
        "# graphs, num_classes = load_data(data_name, use_degree_as_tag)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmG5G1TgdTak"
      },
      "source": [
        "### GCN_layer.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fOYJcC4rdWlV"
      },
      "outputs": [],
      "source": [
        "# %tensorflow_version 1.x\n",
        "!pip install tensorflow==1.15.0\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "import numpy as np\n",
        "\n",
        "'''\n",
        "    Thomas N. Kipf, Max Welling. 2017. Semi-Supervised Classification with Graph Convolutional Networks. ICLR.\n",
        "    Modified from https://github.com/tkipf/gcn/blob/master/gcn/layers.py\n",
        "'''\n",
        "\n",
        "def uniform(shape, scale=0.05, name=None):\n",
        "    \"\"\"Uniform init.\"\"\"\n",
        "    initial = tf.random_uniform(shape, minval=-scale, maxval=scale, dtype=tf.float32)\n",
        "    return tf.Variable(initial, name=name)\n",
        "\n",
        "def glorot(shape, name=None):\n",
        "    \"\"\"Glorot & Bengio (AISTATS 2010) init.\"\"\"\n",
        "    init_range = np.sqrt(6.0 / (shape[0] + shape[1]))\n",
        "    initial = tf.compat.v1.random_uniform(shape, minval=-init_range, maxval=init_range, dtype=tf.float32)\n",
        "    return tf.Variable(initial, name=name)\n",
        "\n",
        "def zeros(shape, name=None):\n",
        "    \"\"\"All zeros.\"\"\"\n",
        "    initial = tf.zeros(shape, dtype=tf.float32)\n",
        "    return tf.Variable(initial, name=name)\n",
        "\n",
        "def ones(shape, name=None):\n",
        "    \"\"\"All ones.\"\"\"\n",
        "    initial = tf.ones(shape, dtype=tf.float32)\n",
        "    return tf.Variable(initial, name=name)\n",
        "\n",
        "flags = tf.compat.v1.flags\n",
        "FLAGS = flags.FLAGS\n",
        "\n",
        "# global unique layer ID dictionary for layer name assignment\n",
        "_LAYER_UIDS = {}\n",
        "\n",
        "def get_layer_uid(layer_name=''):\n",
        "    \"\"\"Helper function, assigns unique layer IDs.\"\"\"\n",
        "    if layer_name not in _LAYER_UIDS:\n",
        "        _LAYER_UIDS[layer_name] = 1\n",
        "        return 1\n",
        "    else:\n",
        "        _LAYER_UIDS[layer_name] += 1\n",
        "        return _LAYER_UIDS[layer_name]\n",
        "\n",
        "def sparse_dropout(x, keep_prob, noise_shape):\n",
        "    \"\"\"Dropout for sparse tensors.\"\"\"\n",
        "    random_tensor = keep_prob\n",
        "    random_tensor += tf.compat.v1.random_uniform(noise_shape)\n",
        "    dropout_mask = tf.cast(tf.floor(random_tensor), dtype=tf.bool)\n",
        "    pre_out = tf.compat.v1.sparse_retain(x, dropout_mask)\n",
        "    return pre_out * (1./keep_prob)\n",
        "\n",
        "def dot(x, y, sparse=False):\n",
        "    \"\"\"Wrapper for tf.matmul (sparse vs dense).\"\"\"\n",
        "    if sparse:\n",
        "        res = tf.compat.v1.sparse_tensor_dense_matmul(x, y)\n",
        "    else:\n",
        "        res = tf.matmul(x, y)\n",
        "    return res\n",
        "\n",
        "class Layer(object):\n",
        "    \"\"\"Base layer class. Defines basic API for all layer objects.\n",
        "    Implementation inspired by keras (http://keras.io).\n",
        "\n",
        "    # Properties\n",
        "        name: String, defines the variable scope of the layer.\n",
        "        logging: Boolean, switches Tensorflow histogram logging on/off\n",
        "\n",
        "    # Methods\n",
        "        _call(inputs): Defines computation graph of layer\n",
        "            (i.e. takes input, returns output)\n",
        "        __call__(inputs): Wrapper for _call()\n",
        "        _log_vars(): Log all variables\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        allowed_kwargs = {'name', 'logging'}\n",
        "        for kwarg in kwargs.keys():\n",
        "            assert kwarg in allowed_kwargs, 'Invalid keyword argument: ' + kwarg\n",
        "        name = kwargs.get('name')\n",
        "        if not name:\n",
        "            layer = self.__class__.__name__.lower()\n",
        "            name = layer + '_' + str(get_layer_uid(layer))\n",
        "        self.name = name\n",
        "        self.vars = {}\n",
        "        logging = kwargs.get('logging', False)\n",
        "        self.logging = logging\n",
        "        self.sparse_inputs = False\n",
        "\n",
        "    def _call(self, inputs):\n",
        "        return inputs\n",
        "\n",
        "    def __call__(self, inputs):\n",
        "        with tf.name_scope(self.name):\n",
        "            if self.logging and not self.sparse_inputs:\n",
        "                tf.compat.v1.summary.histogram(self.name + '/inputs', inputs)\n",
        "            outputs = self._call(inputs)\n",
        "            if self.logging:\n",
        "                tf.compat.v1.summary.histogram(self.name + '/outputs', outputs)\n",
        "            return outputs\n",
        "\n",
        "    def _log_vars(self):\n",
        "        for var in self.vars:\n",
        "            tf.compat.v1.summary.histogram(self.name + '/vars/' + var, self.vars[var])\n",
        "\n",
        "\n",
        "class GraphConvolution(Layer):\n",
        "    \"\"\"Graph convolution layer.\"\"\"\n",
        "    def __init__(self, input_dim, output_dim, placeholders, dropout=0.,\n",
        "                 sparse_inputs=False, act=tf.nn.relu, bias=False,\n",
        "                 featureless=False, **kwargs):\n",
        "        super(GraphConvolution, self).__init__(**kwargs)\n",
        "\n",
        "        if dropout:\n",
        "            self.dropout = placeholders['dropout']\n",
        "        else:\n",
        "            self.dropout = 0.\n",
        "\n",
        "        self.act = act\n",
        "        self.adj = placeholders['adj']\n",
        "        self.sparse_inputs = sparse_inputs\n",
        "        self.featureless = featureless\n",
        "        self.bias = bias\n",
        "\n",
        "        # helper variable for sparse dropout\n",
        "        self.num_features_nonzero = placeholders['num_features_nonzero']\n",
        "\n",
        "        with tf.compat.v1.variable_scope(self.name + '_vars'):\n",
        "            self.vars['weights'] = glorot([input_dim, output_dim], name='weights')\n",
        "            if self.bias:\n",
        "                self.vars['bias'] = zeros([output_dim], name='bias')\n",
        "\n",
        "        if self.logging:\n",
        "            self._log_vars()\n",
        "\n",
        "    def _call(self, inputs):\n",
        "        x = inputs\n",
        "\n",
        "        # dropout\n",
        "        if self.sparse_inputs:\n",
        "            x = sparse_dropout(x, 1-self.dropout, self.num_features_nonzero)\n",
        "        else:\n",
        "            x = tf.nn.dropout(x, 1-self.dropout)\n",
        "\n",
        "        # convolve\n",
        "        if not self.featureless:\n",
        "            pre_sup = dot(x, self.vars['weights'], sparse=self.sparse_inputs)\n",
        "        else:\n",
        "            pre_sup = self.vars['weights']\n",
        "        output = dot(self.adj, pre_sup, sparse=True)\n",
        "\n",
        "        # bias\n",
        "        if self.bias:\n",
        "            output += self.vars['bias']\n",
        "\n",
        "        return self.act(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FeAr2MvutWR"
      },
      "source": [
        "### Model_unsup_gcn.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hgNzN7G8uxYz"
      },
      "outputs": [],
      "source": [
        "class GCN_graph_cls(object):\n",
        "    def __init__(self, feature_dim_size, hidden_size, num_GNN_layers, num_sampled, vocab_size):\n",
        "        # Placeholders for input, output\n",
        "        self.Adj_block = tf.compat.v1.sparse_placeholder(tf.float32, [None, None], name=\"Adj_block\")\n",
        "        self.X_concat = tf.compat.v1.sparse_placeholder(tf.float32, [None, feature_dim_size], name=\"X_concat\")\n",
        "        self.num_features_nonzero = tf.compat.v1.placeholder(tf.int32, name=\"num_features_nonzero\")\n",
        "        self.dropout = tf.compat.v1.placeholder(tf.float32, name=\"dropout\")\n",
        "        self.input_y = tf.compat.v1.placeholder(tf.int32, [None, 1], name=\"input_y\")\n",
        "\n",
        "        self.placeholders = {\n",
        "            'adj': self.Adj_block,\n",
        "            'dropout': self.dropout,\n",
        "            'num_features_nonzero': self.num_features_nonzero\n",
        "        }\n",
        "\n",
        "        self.input = self.X_concat   # set hidden_size = feature_dim_size if not tuning sizes of hidden stacked layers\n",
        "        in_hidden_size = feature_dim_size\n",
        "        print('in_hidden_size = ', in_hidden_size)\n",
        "        self.output_vectors = []\n",
        "        #Construct k GNN layers\n",
        "        for idx_layer in range(num_GNN_layers):\n",
        "            sparse_inputs = False\n",
        "            if idx_layer == 0:\n",
        "                sparse_inputs = True\n",
        "            gcn_gnn = GraphConvolution(input_dim=in_hidden_size,\n",
        "                                                  output_dim=hidden_size,\n",
        "                                                  placeholders=self.placeholders,\n",
        "                                                  act=tf.nn.relu,\n",
        "                                                  dropout=True,\n",
        "                                                  sparse_inputs=sparse_inputs)\n",
        "\n",
        "            in_hidden_size = hidden_size\n",
        "            \n",
        "            # print('in_hidden_size2 = ', in_hidden_size)\n",
        "            # run --> output --> input for next layer\n",
        "            self.input = gcn_gnn(self.input)\n",
        "\n",
        "            # print('shape = ', self.input.get_shape)\n",
        "            #\n",
        "            self.output_vectors.append(self.input)\n",
        "\n",
        "        self.output_vectors = tf.concat(self.output_vectors, axis=1)\n",
        "        self.output_vectors = tf.nn.dropout(self.output_vectors, 1-self.dropout)\n",
        "\n",
        "        with tf.name_scope(\"embedding\"):\n",
        "            self.embedding_matrix = glorot([vocab_size, hidden_size*num_GNN_layers], name='node_embeddings')\n",
        "            self.softmax_biases = tf.Variable(tf.zeros([vocab_size]))\n",
        "\n",
        "        self.total_loss = tf.reduce_mean(\n",
        "            tf.nn.sampled_softmax_loss(weights=self.embedding_matrix, biases=self.softmax_biases,\n",
        "                                       inputs=self.output_vectors, labels=self.input_y, num_sampled=num_sampled, num_classes=vocab_size))\n",
        "\n",
        "        self.saver = tf.compat.v1.train.Saver(tf.global_variables(), max_to_keep=500)\n",
        "        tf.logging.info('Seting up the main structure')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUZOCPYKMOaw"
      },
      "source": [
        "### Calculate Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g5nsCyUnMSGY"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn import preprocessing\n",
        "\n",
        "def cal_acc_only(predicted, gt):\n",
        "  correct = 0\n",
        "  for i in range(len(predicted)):\n",
        "    if(predicted[i] == gt[i]):\n",
        "      correct += 1\n",
        "\n",
        "  acc = correct/len(predicted)\n",
        "\n",
        "  return acc\n",
        "\n",
        "def calculate_acc(predicted, gt):\n",
        "  correct = 0\n",
        "  for i in range(len(predicted)):\n",
        "    if(predicted[i] == gt[i]):\n",
        "      correct += 1\n",
        "\n",
        "  acc = correct/len(predicted)\n",
        "\n",
        "  lb = preprocessing.LabelBinarizer()\n",
        "  lb.fit(gt)\n",
        "\n",
        "  gt_binary = lb.transform(gt)\n",
        "  predicted_binary = lb.transform(predicted)\n",
        "\n",
        "  auc = roc_auc_score(gt_binary, predicted_binary, average = 'macro')\n",
        "  precision, recall, f1score, support = precision_recall_fscore_support(gt_binary, predicted_binary, average = 'macro')\n",
        "\n",
        "  A = classification_report(predicted, gt, digits = 4)\n",
        "  # print(A)\n",
        "\n",
        "\n",
        "  return acc, auc, precision, recall, f1score"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GCN Graph Setup Utility"
      ],
      "metadata": {
        "id": "EIIaeFoiMg3o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#! /usr/bin/env python\n",
        "!pip install keras\n",
        "!pip install keract\n",
        "\n",
        "def FCN_classify(X_train, X_test, label_train, label_test, num_classes):\n",
        "  model = models.Sequential()\n",
        "  model.add(layers.Dense(128, activation='relu'))\n",
        "  model.add(layers.Dense(num_classes, activation='softmax'))\n",
        "  model.summary()\n",
        "\n",
        "  model.compile(optimizer='adam',\n",
        "                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                metrics=['accuracy'])\n",
        "  \n",
        "  model.fit(X_train, label_train, epochs = 166, verbose =0)\n",
        "\n",
        "  predictions = model.predict(X_test)\n",
        "\n",
        "  predicted_labales = np.argmax(predictions, axis = 1)\n",
        "\n",
        "  # print(' acc ', acc, ' auc ', auc) \n",
        "  return predicted_labales\n",
        "\n",
        "  \n",
        "\n",
        "def get_Adj_matrix(batch_graph):\n",
        "    edge_mat_list = []\n",
        "    start_idx = [0]\n",
        "    for i, graph in enumerate(batch_graph):\n",
        "        start_idx.append(start_idx[i] + len(graph.g))\n",
        "        edge_mat_list.append(graph.edge_mat + start_idx[i])\n",
        "\n",
        "    Adj_block_idx = np.concatenate(edge_mat_list, 1)\n",
        "    Adj_block_elem = np.ones(Adj_block_idx.shape[1])\n",
        "\n",
        "    #self-loop\n",
        "    num_node = start_idx[-1]\n",
        "    self_loop_edge = np.array([range(num_node), range(num_node)])\n",
        "    elem = np.ones(num_node)\n",
        "    Adj_block_idx = np.concatenate([Adj_block_idx, self_loop_edge], 1)\n",
        "    Adj_block_elem = np.concatenate([Adj_block_elem, elem], 0)\n",
        "\n",
        "    Adj_block = coo_matrix((Adj_block_elem, Adj_block_idx), shape=(num_node, num_node))\n",
        "\n",
        "    return Adj_block\n",
        "\n",
        "def get_graphpool(batch_graph):\n",
        "    start_idx = [0]\n",
        "    # compute the padded neighbor list\n",
        "    for i, graph in enumerate(batch_graph):\n",
        "        start_idx.append(start_idx[i] + len(graph.g))\n",
        "\n",
        "    idx = []\n",
        "    elem = []\n",
        "    for i, graph in enumerate(batch_graph):\n",
        "        elem.extend([1] * len(graph.g))\n",
        "        idx.extend([[i, j] for j in range(start_idx[i], start_idx[i + 1], 1)])\n",
        "\n",
        "    elem = np.array(elem)\n",
        "    idx = np.array(idx)\n",
        "\n",
        "    graph_pool = coo_matrix((elem, (idx[:, 0], idx[:, 1])), shape=(len(batch_graph), start_idx[-1]))\n",
        "    return graph_pool\n",
        "  \n",
        "def get_idx_nodes(selected_graph_idx):\n",
        "    idx_nodes = [np.where(graph_pool.getrow(i).toarray()[0] == 1)[0] for i in selected_graph_idx]\n",
        "    idx_nodes = np.reshape(np.concatenate(idx_nodes), (-1, 1))\n",
        "    return idx_nodes\n",
        "\n",
        "def get_batch_data(batch_graph):\n",
        "    # features\n",
        "    X_concat = np.concatenate([graph.node_features for graph in batch_graph], 0)\n",
        "    if \"REDDIT\" in args.dataset:\n",
        "        X_concat = np.tile(X_concat, feature_dim_size) #[1,1,1,1]\n",
        "        X_concat = X_concat * 0.01\n",
        "\n",
        "    X_concat = coo_matrix(X_concat)\n",
        "    X_concat = sparse_to_tuple(X_concat)\n",
        "    # adj\n",
        "    Adj_block = get_Adj_matrix(batch_graph)\n",
        "    Adj_block = sparse_to_tuple(Adj_block)\n",
        "\n",
        "    num_features_nonzero = X_concat[1].shape\n",
        "    return Adj_block, X_concat, num_features_nonzero\n",
        "\n",
        "class Batch_Loader(object):\n",
        "    def __call__(self):\n",
        "        selected_idx = np.random.permutation(len(graphs))[:args.batch_size]\n",
        "        batch_graph = [graphs[idx] for idx in selected_idx]\n",
        "        Adj_block, X_concat, num_features_nonzero = get_batch_data(batch_graph)\n",
        "        idx_nodes = get_idx_nodes(selected_idx)\n",
        "        return Adj_block, X_concat, num_features_nonzero, idx_nodes\n",
        "\n"
      ],
      "metadata": {
        "id": "JrMVH4wrMgF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbpx67cimwyu"
      },
      "source": [
        "### Parameters\n",
        "\n",
        "Most parameters here will change parameters used in training. Only \"dataset\" unused and instead can be changed in the function to be used for looping through all data sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dGVwHximwD2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import datetime\n",
        "import pickle as cPickle\n",
        "from argparse import ArgumentParser, ArgumentDefaultsHelpFormatter\n",
        "\n",
        "\n",
        "# Parameters\n",
        "# ==================================================\n",
        "\n",
        "parser = ArgumentParser(\"GCN_Unsup\", formatter_class=ArgumentDefaultsHelpFormatter, conflict_handler='resolve')\n",
        "\n",
        "parser.add_argument(\"--run_folder\", default=\"../\", help=\"\")\n",
        "parser.add_argument(\"--dataset\", default=\"dataset1\", help=\"Name of the dataset.\")\n",
        "parser.add_argument(\"--learning_rate\", default=0.001, type=float, help=\"Learning rate\")\n",
        "parser.add_argument(\"--batch_size\", default = 1, type=int, help=\"Batch Size\")\n",
        "parser.add_argument(\"--num_epochs\", default = 50, type=int, help=\"Number of training epochs\")\n",
        "parser.add_argument(\"--saveStep\", default=1, type=int, help=\"\")\n",
        "parser.add_argument(\"--allow_soft_placement\", default=False, type=bool, help=\"Allow device soft device placement\")\n",
        "parser.add_argument(\"--log_device_placement\", default=False, type=bool, help=\"Log placement of ops on devices\")\n",
        "parser.add_argument(\"--model_name\", default='MUTAG', help=\"\")\n",
        "parser.add_argument(\"--dropout\", default=0.5, type=float, help=\"Dropout\")\n",
        "parser.add_argument(\"--num_GNN_layers\", default=6, type=int, help=\"Number of stacked layers\")\n",
        "parser.add_argument(\"--hidden_size\", default=64, type=int, help=\"size of hidden layers\")\n",
        "parser.add_argument('--num_sampled', default=512, type=int, help='')\n",
        "# args = parser.parse_args()\n",
        "args = parser.parse_args(args = [])\n",
        "\n",
        "print(args)\n",
        "\n",
        "# Load data\n",
        "print(\"Loading data...\")\n",
        "\n",
        "use_degree_as_tag = False\n",
        "\n",
        "if args.dataset == 'COLLAB' or args.dataset == 'IMDBBINARY' or args.dataset == 'IMDBMULTI':\n",
        "    use_degree_as_tag = True\n",
        "graphs, num_classes = load_data(args.dataset, use_degree_as_tag)\n",
        "feature_dim_size = graphs[0].node_features.shape[1]\n",
        "graph_labels = np.array([graph.label for graph in graphs])\n",
        "if \"REDDIT\" in args.dataset:\n",
        "    feature_dim_size = 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zIFPXFbDn_N"
      },
      "source": [
        "### Train HBcompare\n",
        "\n",
        "run_HBCompare_training() function runs the training of graph features in the GCN. \n",
        "\n",
        "Overview:\n",
        "- Input: The function expects that a dataset has already been loaded, the graphs have been loaded as a graph_pool, and a batch loader created as batch_nodes.\n",
        "\n",
        "- Output: Performance scores. More scores can be added to be appended to an array while running through iterations, and then printed at the end.\n",
        "\n",
        "1. The GCN first creates a session and sets up the GCN architecture\n",
        "2. An outer for loop goes through all 50 epochs\n",
        "3. 2 inner for loops:\n",
        "  - First loop does unsupervised training through all graph batches and updates the loss function\n",
        "  - 2nd loop does a 5-fold CV and gives the average and stdev of all folds\n",
        "\n",
        "*** Warning: google colab times out after a while if not constantly interacting, and 10 iteration loops for a dataset can take 2 hrs (650s * 10 iterations) -> for all 5 datasets can take 7-8 hrs. Recommended to run one dataset at a time and make sure to check captcha every once in a while, or run on local jupyter notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nhgC-JN2DqA2"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "np.random.seed(123)\n",
        "tf.compat.v1.set_random_seed(123)\n",
        "\n",
        "import os\n",
        "import time\n",
        "import datetime\n",
        "import pickle as cPickle\n",
        "from argparse import ArgumentParser, ArgumentDefaultsHelpFormatter\n",
        "from scipy.sparse import coo_matrix\n",
        "import statistics\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import learning_curve\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import scipy.io\n",
        "from tensorflow.keras import backend as K\n",
        "from keract import get_activations\n",
        "import keract\n",
        "\n",
        "### Datasets to loop through for training in the list - \n",
        "### Warning: google colab times out after a while, and 10 iteration loops for a dataset can take 2 hrs (650s * 10 iterations) -> for all 5 datasets can take 7-8 hrs\n",
        "\n",
        "# all_datasets = [\"dataset1\",\"dataset2\",\"dataset3\",\"dataset4\",\"dataset5\"]\n",
        "all_datasets = [\"dataset2\"]\n",
        "\n",
        "\n",
        "# Training\n",
        "# ==================================================\n",
        "def run_HBCompare_training():\n",
        "  with tf.Graph().as_default():\n",
        "      # Setup tensorflow session\n",
        "      session_conf = tf.compat.v1.ConfigProto(allow_soft_placement=args.allow_soft_placement, log_device_placement=args.log_device_placement)\n",
        "      session_conf.gpu_options.allow_growth = True\n",
        "      sess = tf.compat.v1.Session(config=session_conf)\n",
        "      with sess.as_default():\n",
        "          global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
        "          print(\"Feature_dim_size = \", feature_dim_size)\n",
        "          print(\"Hidden_size = \", args.hidden_size)\n",
        "          print(\"Graph_pool_shape = \", graph_pool.shape[0], \" ... \", graph_pool.shape[1])\n",
        "          print(\"num_sampled = \", args.num_sampled)\n",
        "\n",
        "          unsup_gcn = GCN_graph_cls(feature_dim_size=feature_dim_size,\n",
        "                        hidden_size=args.hidden_size,\n",
        "                        num_GNN_layers=args.num_GNN_layers,\n",
        "                        vocab_size=graph_pool.shape[1],\n",
        "                        num_sampled=args.num_sampled,\n",
        "                        )\n",
        "\n",
        "          # Define Training procedure\n",
        "          optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=args.learning_rate)\n",
        "          grads_and_vars = optimizer.compute_gradients(unsup_gcn.total_loss)\n",
        "          train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
        "\n",
        "          # Folder to save run logs\n",
        "          out_dir = os.path.abspath(os.path.join(args.run_folder, \"../runs_GCN_UnSup\", args.model_name))\n",
        "          print(\"Writing to {}\\n\".format(out_dir))\n",
        "          # print(\"Data set: \", args.dataset)\n",
        "\n",
        "          # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
        "          checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
        "          checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
        "          if not os.path.exists(checkpoint_dir):\n",
        "              os.makedirs(checkpoint_dir)\n",
        "\n",
        "          # Initialize all variables\n",
        "          sess.run(tf.compat.v1.global_variables_initializer())\n",
        "          graph = tf.compat.v1.get_default_graph()\n",
        "\n",
        "          # Setup training step for unsupervised training of graph batches\n",
        "          def train_step(Adj_block, X_concat, num_features_nonzero, idx_nodes):\n",
        "              feed_dict = {\n",
        "                  unsup_gcn.Adj_block: Adj_block,\n",
        "                  unsup_gcn.X_concat: X_concat,\n",
        "                  unsup_gcn.num_features_nonzero: num_features_nonzero,\n",
        "                  unsup_gcn.dropout: args.dropout,\n",
        "                  unsup_gcn.input_y:idx_nodes\n",
        "              }\n",
        "              _, step, loss = sess.run([train_op, global_step, unsup_gcn.total_loss], feed_dict)\n",
        "              return loss\n",
        "\n",
        "          # write_acc = open(checkpoint_prefix + '_acc.txt', 'w')\n",
        "          max_acc = 0.0\n",
        "          idx_epoch = 0\n",
        "          num_batches_per_epoch = int((len(graphs) - 1) / args.batch_size) + 1\n",
        "          predicted_labels = []\n",
        "          acc_all = []\n",
        "          auc_all = []\n",
        "          f1score_all = []\n",
        "\n",
        "          mean_each_epoch = []\n",
        "          std_each_epoch = []\n",
        "          time_each_epoch = []\n",
        "\n",
        "          start_time = time.time()\n",
        "\n",
        "          # Start epochs from 1-50\n",
        "          for epoch in range(1, args.num_epochs+1):\n",
        "\n",
        "              # feature update using loss function - loss update is unsupervised\n",
        "              loss = 0\n",
        "              for _ in range(num_batches_per_epoch):\n",
        "                  Adj_block, X_concat, num_features_nonzero, idx_nodes = batch_nodes()\n",
        "                  loss += train_step(Adj_block, X_concat, num_features_nonzero, idx_nodes)\n",
        "                  # current_step = tf.compat.v1.train.global_step(sess, global_step)\n",
        "              print(loss)\n",
        "              # It will give tensor object\n",
        "              node_embeddings = graph.get_tensor_by_name('embedding/node_embeddings:0')\n",
        "              node_embeddings = sess.run(node_embeddings)\n",
        "              graph_embeddings = graph_pool.dot(node_embeddings)\n",
        "              # Keep acc for all CV folds to get avg\n",
        "              acc_10folds = []\n",
        "\n",
        "              # Do 5 fold cross validation per epoch\n",
        "              for fold_idx in range(0,5):\n",
        "\n",
        "                  # Setting up train/test split for 5fold-CV\n",
        "                  train_idx, test_idx = separate_data_idx(graphs, fold_idx)\n",
        "                  train_graph_embeddings = graph_embeddings[train_idx]\n",
        "                  test_graph_embeddings = graph_embeddings[test_idx]\n",
        "                  train_labels = graph_labels[train_idx]\n",
        "                  test_labels = graph_labels[test_idx]\n",
        "\n",
        "                  # Logistic regression model for training\n",
        "                  cls = LogisticRegression(tol=0.001, max_iter = 2000)\n",
        "                  # cls = SVC()\n",
        "                  trained_model = cls.fit(train_graph_embeddings, train_labels)\n",
        "\n",
        "                  # Model training setup\n",
        "                  predicted = cls.predict(test_graph_embeddings)\n",
        "                  ACC = cal_acc_only(predicted, test_labels)\n",
        "                  acc_10folds.append(ACC)\n",
        "\n",
        "                  # On last epoch, save final scores\n",
        "                  if(epoch == args.num_epochs):\n",
        "                    predicted = cls.predict(test_graph_embeddings)\n",
        "                    predicted_labels.append(predicted)\n",
        "\n",
        "                    acc, auc, precision, recall, f1score = calculate_acc(predicted, test_labels)\n",
        "                    acc_all.append(acc)\n",
        "                    auc_all.append(auc)\n",
        "                    f1score_all.append(f1score)\n",
        "\n",
        "                  # print('epoch ', epoch, ' fold ', fold_idx, ' acc ', ACC)\n",
        "\n",
        "\n",
        "              # After CV folds, get mean accuracy and append to list - organized per epoch\n",
        "              mean_10folds = statistics.mean(acc_10folds)\n",
        "              std_10folds = statistics.stdev(acc_10folds)\n",
        "\n",
        "              print('epoch ', epoch, ' mean: ', str(mean_10folds*100), ' std: ', str(std_10folds*100))\n",
        "\n",
        "              mean_each_epoch.append(str(mean_10folds*100))\n",
        "              std_each_epoch.append(str(std_10folds*100))\n",
        "              time_each_epoch.append(time.time()-start_time)\n",
        "\n",
        "  end_time = time.time()\n",
        "  total_time = end_time - start_time\n",
        "  print(\"--- %s seconds ---\" % total_time)\n",
        "\n",
        "  print('acc_all', acc_all)   \n",
        "  print('f1_all', f1score_all)\n",
        "  print('auc_all', auc_all)  \n",
        "\n",
        "  return acc_all, auc_all, f1score_all, mean_each_epoch, std_each_epoch, time_each_epoch, total_time\n",
        "\n",
        "\n",
        "all_data = []\n",
        "iter = 5\n",
        "for d in range(len(all_datasets)):\n",
        "  all_acc = []\n",
        "  all_auc = []\n",
        "  all_f1score = []\n",
        "  all_mean_each_epoch = []\n",
        "  all_std_each_epoch = []\n",
        "  all_time_each_epoch = []\n",
        "  all_time = []\n",
        "  for i in range(iter):\n",
        "    graphs, num_classes = load_data(all_datasets[d], use_degree_as_tag)\n",
        "    feature_dim_size = graphs[0].node_features.shape[1]\n",
        "    graph_labels = np.array([graph.label for graph in graphs])\n",
        "\n",
        "    graph_pool = get_graphpool(graphs)\n",
        "    # print(\"Graph_pool :\", graph_pool)\n",
        "    batch_nodes = Batch_Loader()\n",
        "\n",
        "    print(\"Loading data... finished!\")\n",
        "    print(all_datasets[d], \" loaded\")\n",
        "\n",
        "    acc_all,auc_all, f1score_all, mean_each_epoch, std_each_epoch, time_each_epoch, total_time = run_HBCompare_training()\n",
        "    all_acc.append(acc_all)\n",
        "    all_auc.append(auc_all)\n",
        "    all_f1score.append(f1score_all)\n",
        "    all_mean_each_epoch.append(mean_each_epoch)\n",
        "    all_std_each_epoch.append(std_each_epoch)\n",
        "    all_time_each_epoch.append(time_each_epoch)\n",
        "    all_time.append(total_time)\n",
        "  \n",
        "  all_data.append(all_acc)\n",
        "  all_data.append(all_auc)\n",
        "  all_data.append(all_f1score)\n",
        "  all_data.append(all_mean_each_epoch)\n",
        "  all_data.append(all_std_each_epoch)\n",
        "  all_data.append(all_time_each_epoch)\n",
        "  all_data.append(all_time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "djSwO7X4Fpzw"
      },
      "outputs": [],
      "source": [
        "# Organizing results from HBcompare\n",
        "for i in range(len(all_datasets)):\n",
        "  all_acc = all_data[i+0]\n",
        "  all_auc = all_data[i+1]\n",
        "  all_f1score = all_data[i+2]\n",
        "  all_mean_each_epoch = all_data[i+3]\n",
        "  all_std_each_epoch = all_data[i+4]\n",
        "  all_time_each_epoch = all_data[i+5]\n",
        "  all_time = all_data[i+6]\n",
        "\n",
        "  print(\"ACC:\", np.mean(all_acc), \" STD:\", np.std(all_acc))\n",
        "  print(\"AUC:\", np.mean(all_auc), \" STD:\", np.std(all_auc))\n",
        "  print(\"F1:\", np.mean(all_f1score), \" STD:\", np.std(all_f1score))\n",
        "\n",
        "  print(all_mean_each_epoch)\n",
        "  array_acc = []\n",
        "  array_std = []\n",
        "  array_time = []\n",
        "\n",
        "  for i in range(args.num_epochs):\n",
        "    sum_mean_acc = 0\n",
        "    sum_std = 0\n",
        "    sum_time = 0\n",
        "    for j in range(iter):\n",
        "      sum_mean_acc = sum_mean_acc + float(all_mean_each_epoch[j][i])\n",
        "      sum_std = sum_std + float(all_std_each_epoch[j][i])\n",
        "      sum_time = sum_time + float(all_time_each_epoch[j][i])\n",
        "    array_acc.append(sum_mean_acc/iter)\n",
        "    array_std.append(sum_std/iter)\n",
        "    array_time.append(sum_time/iter)\n",
        "  \n",
        "  # Plot time vs accuracy \n",
        "  plt.figure(i)\n",
        "  plt.plot(array_time, array_acc)\n",
        "  plt.xlabel(\"Time (s)\")\n",
        "  plt.ylabel(\"Accuracy\")\n",
        "  plt.yticks(np.arange(75,105,5))\n",
        "  plt.show()\n",
        "\n",
        "  # Plot epoch vs accuracy\n",
        "  plt.figure(i+1)\n",
        "  plt.plot(range(0,50), array_acc)\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.ylabel(\"Accuracy\")\n",
        "  plt.yticks(np.arange(75,105,5))\n",
        "  plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot time vs accuracy and epoch vs accuracy figures"
      ],
      "metadata": {
        "id": "lUMIw30peJzh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save arrays to a csv file\n",
        "\n",
        "- rewrite_array() rewrites an embedded array to a single array format for writing to csv"
      ],
      "metadata": {
        "id": "1-IRNIrtdi1g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5BTbmH2JQGGR"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "# Write data to a csv file\n",
        "\n",
        "def rewrite_array(all_array):\n",
        "  new_array = []\n",
        "  for i in range(len(all_array)):\n",
        "    for j in range(len(all_array[i])):\n",
        "      new_array.append(float(all_array[i][j]))\n",
        "  return new_array\n",
        "with open('time_' + args.dataset + '_with_svm.csv', 'w', newline='') as csvfile:\n",
        "    spamwriter = csv.writer(csvfile, delimiter=',',\n",
        "                             quoting=csv.QUOTE_MINIMAL)\n",
        "    spamwriter.writerow(array_acc)\n",
        "    spamwriter.writerow(array_time)\n",
        "    spamwriter.writerow(array_std)\n",
        "    spamwriter.writerow(rewrite_array(all_f1score))\n",
        "    spamwriter.writerow(rewrite_array(all_auc))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### HBCompare coord data setup\n",
        "\n"
      ],
      "metadata": {
        "id": "Y2wZgmLIxHmg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Vec2Matrix(X):\n",
        "  num_graphs, M_square = X.shape \n",
        "  M_matrix = int(np.sqrt(M_square))\n",
        "  X_matrix = np.zeros([num_graphs, int(np.sqrt(M_square)), int(np.sqrt(M_square))])\n",
        "  for i in range(num_graphs):\n",
        "    Xi = X[i,:]\n",
        "    Xi_matrix = np.reshape(Xi, [M_matrix, M_matrix])\n",
        "    X_matrix[i,:,:] = Xi_matrix\n",
        "\n",
        "  return X_matrix\n",
        "\n",
        "\n",
        "def load_RBF_mat(dataset):\n",
        "  dataset_ = dataset[:-1] + '_' + dataset[-1]\n",
        "  dataset_name = '/content/gdrive/My Drive/Graph_based_methods/Graph_Transformer/dataset/Features_Omar_new/' + dataset_ + '_RBF_plus_connectivity_manual_matrix.mat'\n",
        "\n",
        "  mat = scipy.io.loadmat(dataset_name)\n",
        "  X = mat['X_feature_tensor']\n",
        "\n",
        "  ndims_X = np.ndim(X)\n",
        "  X_feature = X\n",
        "  if(ndims_X == 2):\n",
        "    X_feature = Vec2Matrix(X)\n",
        "  # elif(ndims_X == 3):\n",
        "  #   X_feature = X\n",
        "\n",
        "  label_name = '/content/gdrive/My Drive/Graph_based_methods/Graph_Transformer/dataset/Features_Omar_new/label_'+ dataset_ + '.mat'\n",
        "  mat = scipy.io.loadmat(label_name)\n",
        "  label_all = mat['label_all'][0]\n",
        "\n",
        "  print('*********** dataset_name =', dataset_name,' *******************')\n",
        "  classes = np.unique(label_all)\n",
        "  num_classes = classes.size\n",
        "  print('number of classes: ', num_classes)\n",
        "\n",
        "  return X_feature, label_all\n",
        "\n",
        "X_feature, label_all = load_RBF_mat('dataset1')\n",
        "# print(X_feature)\n",
        "use_degree_as_tag = False\n",
        "\n",
        "\n",
        "def get_coord_Adj_matrix(selected_idx, X_feature, batch_graph):\n",
        "  Adj_block_full = []\n",
        "  for idx in selected_idx:\n",
        "    mat = X_feature[idx]\n",
        "    Adj_block = coo_matrix(mat, shape=(mat.shape[0], mat.shape[1]))\n",
        "  # with np.printoptions(threshold=np.inf):\n",
        "  #     print(Adj_block)\n",
        "  Adj_block = sparse_to_tuple(Adj_block)\n",
        "  return Adj_block\n",
        "\n",
        "\n",
        "selected_idx = np.random.permutation(len(graphs))[:args.batch_size]\n",
        "batch_graph = [graphs[idx] for idx in selected_idx]\n",
        "print(selected_idx)\n",
        "get_coord_Adj_matrix(selected_idx, X_feature, batch_graph)\n",
        "\n",
        "def get_Adj_matrix(batch_graph):\n",
        "    edge_mat_list = []\n",
        "    start_idx = [0]\n",
        "    for i, graph in enumerate(batch_graph):\n",
        "        start_idx.append(start_idx[i] + len(graph.g))\n",
        "        edge_mat_list.append(graph.edge_mat + start_idx[i])\n",
        "\n",
        "    Adj_block_idx = np.concatenate(edge_mat_list, 1)\n",
        "    # print(\"Block_idx:\",Adj_block_idx)\n",
        "    Adj_block_elem = np.ones(Adj_block_idx.shape[1])\n",
        "    # print(\"Block_elem:\", Adj_block_elem.shape)\n",
        "\n",
        "    #self-loop\n",
        "    num_node = start_idx[-1]\n",
        "    self_loop_edge = np.array([range(num_node), range(num_node)])\n",
        "    elem = np.ones(num_node)\n",
        "    Adj_block_idx = np.concatenate([Adj_block_idx, self_loop_edge], 1)\n",
        "    Adj_block_elem = np.concatenate([Adj_block_elem, elem], 0)\n",
        "\n",
        "    # with np.printoptions(threshold=np.inf):  \n",
        "    #   print(\"Block_elem:\", Adj_block_elem)\n",
        "    #   print(\"Block_idx:\",Adj_block_idx)\n",
        "\n",
        "    Adj_block = coo_matrix((Adj_block_elem, Adj_block_idx), shape=(num_node, num_node))\n",
        "    # with np.printoptions(threshold=np.inf):\n",
        "    #   print(Adj_block)\n",
        "    return Adj_block\n",
        "\n",
        "def get_batch_data_coord(batch_graph):\n",
        "    # features\n",
        "    X_concat = np.concatenate([graph.node_features for graph in batch_graph], 0)\n",
        "    if \"REDDIT\" in args.dataset:\n",
        "        X_concat = np.tile(X_concat, feature_dim_size) #[1,1,1,1]\n",
        "        X_concat = X_concat * 0.01\n",
        "\n",
        "    \n",
        "    X_concat = coo_matrix(X_concat)\n",
        "    X_concat = sparse_to_tuple(X_concat)\n",
        "    # adj\n",
        "    Adj_block = get_Adj_matrix(batch_graph)\n",
        "    Adj_block = sparse_to_tuple(Adj_block)\n",
        "    # with np.printoptions(threshold=np.inf):\n",
        "    #   print(Adj_block[0].shape)\n",
        "    #   print(Adj_block[1].shape)\n",
        "\n",
        "    num_features_nonzero = X_concat[1].shape\n",
        "    return Adj_block, X_concat, num_features_nonzero\n",
        "\n",
        "\n",
        "class Batch_Loader_coord(object):\n",
        "    def __call__(self):\n",
        "        selected_idx = np.random.permutation(len(graphs))[:args.batch_size]\n",
        "        batch_graph = [graphs[idx] for idx in selected_idx]\n",
        "        Adj_block, X_concat, num_features_nonzero = get_batch_data_coord(batch_graph)\n",
        "        idx_nodes = get_idx_nodes(selected_idx)\n",
        "\n",
        "        Adj_block_coord = get_coord_Adj_matrix(selected_idx, X_feature, batch_graph)\n",
        "\n",
        "        print(Adj_block[0].shape)\n",
        "        print(Adj_block_coord[0].shape)\n",
        "        print(X_concat[0])\n",
        "        print(X_concat[1])\n",
        "        print(X_concat[])\n",
        "        print(num_features_nonzero)\n",
        "\n",
        "        return Adj_block_, X_concat, num_features_nonzero, idx_nodes"
      ],
      "metadata": {
        "id": "MhKEDn_83MBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run HBCompare Coord\n",
        "\n",
        "Method for HBcompare coordinate features\n",
        "\n",
        "TODO: Fix some bugs"
      ],
      "metadata": {
        "id": "5TvXqgHQW7Zd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "np.random.seed(123)\n",
        "tf.compat.v1.set_random_seed(123)\n",
        "\n",
        "import os\n",
        "import time\n",
        "import datetime\n",
        "import pickle as cPickle\n",
        "from argparse import ArgumentParser, ArgumentDefaultsHelpFormatter\n",
        "from scipy.sparse import coo_matrix\n",
        "import statistics\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import learning_curve\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import scipy.io\n",
        "from tensorflow.keras import backend as K\n",
        "from keract import get_activations\n",
        "import keract\n",
        "\n",
        "\n",
        "# Training\n",
        "# ==================================================\n",
        "def run_HBCompare_training():\n",
        "  with tf.Graph().as_default():\n",
        "      # Setup tensorflow session\n",
        "      session_conf = tf.compat.v1.ConfigProto(allow_soft_placement=args.allow_soft_placement, log_device_placement=args.log_device_placement)\n",
        "      session_conf.gpu_options.allow_growth = True\n",
        "      sess = tf.compat.v1.Session(config=session_conf)\n",
        "      with sess.as_default():\n",
        "          global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
        "          print(\"Feature_dim_size = \", feature_dim_size)\n",
        "          print(\"Hidden_size = \", args.hidden_size)\n",
        "          print(\"Graph_pool_shape = \", graph_pool.shape[0], \" ... \", graph_pool.shape[1])\n",
        "          print(\"num_sampled = \", args.num_sampled)\n",
        "\n",
        "          unsup_gcn = GCN_graph_cls(feature_dim_size=feature_dim_size,\n",
        "                        hidden_size=args.hidden_size,\n",
        "                        num_GNN_layers=args.num_GNN_layers,\n",
        "                        vocab_size=graph_pool.shape[1],\n",
        "                        num_sampled=args.num_sampled,\n",
        "                        )\n",
        "\n",
        "          # Define Training procedure\n",
        "          optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=args.learning_rate)\n",
        "          grads_and_vars = optimizer.compute_gradients(unsup_gcn.total_loss)\n",
        "          train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
        "\n",
        "          # Folder to save run logs\n",
        "          out_dir = os.path.abspath(os.path.join(args.run_folder, \"../runs_GCN_UnSup\", args.model_name))\n",
        "          print(\"Writing to {}\\n\".format(out_dir))\n",
        "          # print(\"Data set: \", args.dataset)\n",
        "\n",
        "          # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
        "          checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
        "          checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
        "          if not os.path.exists(checkpoint_dir):\n",
        "              os.makedirs(checkpoint_dir)\n",
        "\n",
        "          # Initialize all variables\n",
        "          sess.run(tf.compat.v1.global_variables_initializer())\n",
        "          graph = tf.compat.v1.get_default_graph()\n",
        "\n",
        "          def train_step(Adj_block, X_concat, num_features_nonzero, idx_nodes):\n",
        "              feed_dict = {\n",
        "                  unsup_gcn.Adj_block: Adj_block,\n",
        "                  unsup_gcn.X_concat: X_concat,\n",
        "                  unsup_gcn.num_features_nonzero: num_features_nonzero,\n",
        "                  unsup_gcn.dropout: args.dropout,\n",
        "                  unsup_gcn.input_y:idx_nodes\n",
        "              }\n",
        "              _, step, loss = sess.run([train_op, global_step, unsup_gcn.total_loss], feed_dict)\n",
        "              return loss\n",
        "\n",
        "          # write_acc = open(checkpoint_prefix + '_acc.txt', 'w')\n",
        "          max_acc = 0.0\n",
        "          idx_epoch = 0\n",
        "          num_batches_per_epoch = int((len(graphs) - 1) / args.batch_size) + 1\n",
        "          predicted_labels = []\n",
        "          acc_all = []\n",
        "          auc_all = []\n",
        "          f1score_all = []\n",
        "\n",
        "          mean_each_epoch = []\n",
        "          std_each_epoch = []\n",
        "          time_each_epoch = []\n",
        "\n",
        "          start_time = time.time()\n",
        "\n",
        "          # Start epochs from 1-50\n",
        "          # for epoch in range(1, args.num_epochs+1):\n",
        "          for epoch in range(1, 2):\n",
        "\n",
        "              # feature update using loss function - loss update is unsupervised\n",
        "              loss = 0\n",
        "              for _ in range(num_batches_per_epoch):\n",
        "                  Adj_block, X_concat, num_features_nonzero, idx_nodes = batch_nodes()\n",
        "                  loss += train_step(Adj_block, X_concat, num_features_nonzero, idx_nodes)\n",
        "                  # current_step = tf.compat.v1.train.global_step(sess, global_step)\n",
        "              print(loss)\n",
        "              # It will give tensor object\n",
        "              node_embeddings = graph.get_tensor_by_name('embedding/node_embeddings:0')\n",
        "              node_embeddings = sess.run(node_embeddings)\n",
        "              graph_embeddings = graph_pool.dot(node_embeddings)\n",
        "              # Keep acc for all CV folds to get avg\n",
        "              acc_10folds = []\n",
        "\n",
        "              # Do 5 fold cross validation per epoch\n",
        "              for fold_idx in range(0,5):\n",
        "\n",
        "                  # Setting up train/test split for 5fold-CV\n",
        "                  train_idx, test_idx = separate_data_idx(graphs, fold_idx)\n",
        "                  train_graph_embeddings = graph_embeddings[train_idx]\n",
        "                  test_graph_embeddings = graph_embeddings[test_idx]\n",
        "                  train_labels = graph_labels[train_idx]\n",
        "                  test_labels = graph_labels[test_idx]\n",
        "\n",
        "                  # Logistic regression model for training\n",
        "                  # cls = LogisticRegression(tol=0.001, max_iter = 2000)\n",
        "                  cls = SVC()\n",
        "                  trained_model = cls.fit(train_graph_embeddings, train_labels)\n",
        "\n",
        "                  # Model training setup\n",
        "                  predicted = cls.predict(test_graph_embeddings)\n",
        "                  ACC = cal_acc_only(predicted, test_labels)\n",
        "                  acc_10folds.append(ACC)\n",
        "\n",
        "                  # On last epoch, save final scores\n",
        "                  if(epoch == args.num_epochs):\n",
        "                    predicted = cls.predict(test_graph_embeddings)\n",
        "                    predicted_labels.append(predicted)\n",
        "\n",
        "                    acc, auc, precision, recall, f1score = calculate_acc(predicted, test_labels)\n",
        "                    acc_all.append(acc)\n",
        "                    auc_all.append(auc)\n",
        "                    f1score_all.append(f1score)\n",
        "\n",
        "                  # print('epoch ', epoch, ' fold ', fold_idx, ' acc ', ACC)\n",
        "\n",
        "\n",
        "              # After CV folds, get mean accuracy and append to list - organized per epoch\n",
        "              mean_10folds = statistics.mean(acc_10folds)\n",
        "              std_10folds = statistics.stdev(acc_10folds)\n",
        "\n",
        "              print('epoch ', epoch, ' mean: ', str(mean_10folds*100), ' std: ', str(std_10folds*100))\n",
        "\n",
        "              mean_each_epoch.append(str(mean_10folds*100))\n",
        "              std_each_epoch.append(str(std_10folds*100))\n",
        "              time_each_epoch.append(time.time()-start_time)\n",
        "\n",
        "  end_time = time.time()\n",
        "  total_time = end_time - start_time\n",
        "  print(\"--- %s seconds ---\" % total_time)\n",
        "\n",
        "  print('acc_all', acc_all)   \n",
        "  print('f1_all', f1score_all)\n",
        "  print('auc_all', auc_all)  \n",
        "\n",
        "  return acc_all, auc_all, f1score_all, mean_each_epoch, std_each_epoch, time_each_epoch, total_time\n",
        "\n",
        "\n",
        "# all_datasets = [\"dataset1\",\"dataset2\",\"dataset3\",\"dataset4\",\"dataset5\"]\n",
        "all_datasets = [\"dataset1\"]\n",
        "\n",
        "all_acc = []\n",
        "all_auc = []\n",
        "all_f1score = []\n",
        "all_mean_each_epoch = []\n",
        "all_std_each_epoch = []\n",
        "all_time_each_epoch = []\n",
        "all_time = []\n",
        "iter = 1\n",
        "\n",
        "for d in range(len(all_datasets)):\n",
        "  for i in range(iter):\n",
        "    graphs, num_classes = load_data(all_datasets[d], use_degree_as_tag)\n",
        "    feature_dim_size = graphs[0].node_features.shape[1]\n",
        "    graph_labels = np.array([graph.label for graph in graphs])\n",
        "\n",
        "    graph_pool = get_graphpool(graphs)\n",
        "    # print(\"Graph_pool :\", graph_pool)\n",
        "    batch_nodes = Batch_Loader_coord()\n",
        "\n",
        "    # X, label_all = load_RBF_mat(all_datasets[d])\n",
        "\n",
        "    print(\"Loading data... finished!\")\n",
        "    print(all_datasets[d], \" loaded\")\n",
        "\n",
        "    print(\"# of iterations:\", i)\n",
        "    acc_all,auc_all, f1score_all, mean_each_epoch, std_each_epoch, time_each_epoch, total_time = run_HBCompare_training()\n",
        "    all_acc.append(acc_all)\n",
        "    all_auc.append(auc_all)\n",
        "    all_f1score.append(f1score_all)\n",
        "    all_mean_each_epoch.append(mean_each_epoch)\n",
        "    all_std_each_epoch.append(std_each_epoch)\n",
        "    all_time_each_epoch.append(time_each_epoch)\n",
        "    all_time.append(total_time)"
      ],
      "metadata": {
        "id": "OnwwM87gxH8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### HBCompare - Parameter Grid\n",
        "\n",
        "This function was used to test parameter grid for logistic regression. \n",
        "Only done on dataset1 and got the following parameters: \n",
        "- tol=0.001, max_iter = 2000, C=0.1\n",
        "\n"
      ],
      "metadata": {
        "id": "45rV7-v8dVSI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "np.random.seed(123)\n",
        "tf.compat.v1.set_random_seed(123)\n",
        "\n",
        "import os\n",
        "import time\n",
        "import datetime\n",
        "import pickle as cPickle\n",
        "from argparse import ArgumentParser, ArgumentDefaultsHelpFormatter\n",
        "from scipy.sparse import coo_matrix\n",
        "import statistics\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import learning_curve\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "from sklearn import svm\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import scipy.io\n",
        "from tensorflow.keras import backend as K\n",
        "from keract import get_activations\n",
        "import keract\n",
        "\n",
        "\n",
        "# Training\n",
        "# ==================================================\n",
        "def run_HBCompare_training():\n",
        "  with tf.Graph().as_default():\n",
        "      session_conf = tf.compat.v1.ConfigProto(allow_soft_placement=args.allow_soft_placement, log_device_placement=args.log_device_placement)\n",
        "      session_conf.gpu_options.allow_growth = True\n",
        "      sess = tf.compat.v1.Session(config=session_conf)\n",
        "      with sess.as_default():\n",
        "          global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
        "          print(\"Feature_dim_size = \", feature_dim_size)\n",
        "          print(\"Hidden_size = \", args.hidden_size)\n",
        "          print(\"Graph_pool_shape = \", graph_pool.shape[0], \" ... \", graph_pool.shape[1])\n",
        "          print(\"num_sampled = \", args.num_sampled)\n",
        "\n",
        "          unsup_gcn = GCN_graph_cls(feature_dim_size=feature_dim_size,\n",
        "                        hidden_size=args.hidden_size,\n",
        "                        num_GNN_layers=args.num_GNN_layers,\n",
        "                        vocab_size=graph_pool.shape[1],\n",
        "                        num_sampled=args.num_sampled,\n",
        "                        )\n",
        "\n",
        "          # Define Training procedure\n",
        "          optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=args.learning_rate)\n",
        "          grads_and_vars = optimizer.compute_gradients(unsup_gcn.total_loss)\n",
        "          train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
        "\n",
        "          # Folder to save run logs\n",
        "          out_dir = os.path.abspath(os.path.join(args.run_folder, \"../runs_GCN_UnSup\", args.model_name))\n",
        "          print(\"Writing to {}\\n\".format(out_dir))\n",
        "          # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
        "          checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
        "          checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
        "          if not os.path.exists(checkpoint_dir):\n",
        "              os.makedirs(checkpoint_dir)\n",
        "\n",
        "          # Initialize all variables\n",
        "          sess.run(tf.compat.v1.global_variables_initializer())\n",
        "          graph = tf.compat.v1.get_default_graph()\n",
        "\n",
        "          def train_step(Adj_block, X_concat, num_features_nonzero, idx_nodes):\n",
        "              feed_dict = {\n",
        "                  unsup_gcn.Adj_block: Adj_block,\n",
        "                  unsup_gcn.X_concat: X_concat,\n",
        "                  unsup_gcn.num_features_nonzero: num_features_nonzero,\n",
        "                  unsup_gcn.dropout: args.dropout,\n",
        "                  unsup_gcn.input_y:idx_nodes\n",
        "              }\n",
        "              _, step, loss = sess.run([train_op, global_step, unsup_gcn.total_loss], feed_dict)\n",
        "              return loss\n",
        "\n",
        "          # write_acc = open(checkpoint_prefix + '_acc.txt', 'w')\n",
        "          max_acc = 0.0\n",
        "          idx_epoch = 0\n",
        "          num_batches_per_epoch = int((len(graphs) - 1) / args.batch_size) + 1\n",
        "          predicted_labels = []\n",
        "          acc_all = []\n",
        "          acc_test_all = []\n",
        "          acc_train_all = []\n",
        "\n",
        "          mean_each_epoch = []\n",
        "\n",
        "\n",
        "          start_time = time.time()\n",
        "\n",
        "          # Start epochs from 1-50\n",
        "          for epoch in range(1, args.num_epochs+51):\n",
        "\n",
        "              # feature update using loss function\n",
        "              loss = 0\n",
        "              for _ in range(num_batches_per_epoch):\n",
        "                  Adj_block, X_concat, num_features_nonzero, idx_nodes = batch_nodes()\n",
        "                  loss += train_step(Adj_block, X_concat, num_features_nonzero, idx_nodes)\n",
        "                  # current_step = tf.compat.v1.train.global_step(sess, global_step)\n",
        "              print(loss)\n",
        "              # It will give tensor object\n",
        "              node_embeddings = graph.get_tensor_by_name('embedding/node_embeddings:0')\n",
        "              node_embeddings = sess.run(node_embeddings)\n",
        "              graph_embeddings = graph_pool.dot(node_embeddings)\n",
        "\n",
        "              acc_10folds = []\n",
        "              trainacc_10folds = []\n",
        "\n",
        "              # Do 5 fold cross validation per epoch\n",
        "              for fold_idx in range(0,5):\n",
        "                  # Setting up train/test split for 5fold-CV\n",
        "                  train_idx, test_idx = separate_data_idx(graphs, fold_idx)\n",
        "                  train_graph_embeddings = graph_embeddings[train_idx]\n",
        "                  test_graph_embeddings = graph_embeddings[test_idx]\n",
        "                  train_labels = graph_labels[train_idx]\n",
        "                  test_labels = graph_labels[test_idx]\n",
        "\n",
        "\n",
        "                  # Logistic regression model for training\n",
        "\n",
        "                  cls = LogisticRegression(tol=0.001, max_iter = 2000, C=0.01,)\n",
        "                  # cls = svm.SVC()\n",
        "                  trained_model = cls.fit(train_graph_embeddings, train_labels)\n",
        "\n",
        "                  predicted_train = cls.predict(train_graph_embeddings)\n",
        "                  ACC_train = cal_acc_only(predicted_train, train_labels)\n",
        "                  trainacc_10folds.append(ACC_train)\n",
        "\n",
        "                  predicted_test = cls.predict(test_graph_embeddings)\n",
        "                  ACC = cal_acc_only(predicted_test, test_labels)\n",
        "                  acc_10folds.append(ACC)\n",
        "                  \n",
        "                  print(LogisticRegression.get_params)\n",
        "                  print(\"Starting pipeline\")\n",
        "                  lr_pipe = Pipeline([('mms', MinMaxScaler()),('lr', LogisticRegression())])\n",
        "                  params = [{'lr__C': [0.001, 0.01, 0.1, 1.0, 10], 'lr__max_iter':[100,500,1000,2000], 'lr__tol':[0.0001, 0.001, 0.01]}]\n",
        "\n",
        "                  gs_gcn = GridSearchCV(lr_pipe, param_grid=params, scoring='accuracy', cv=2)\n",
        "                  gs_gcn.fit(train_graph_embeddings, train_labels)\n",
        "                  print(gs_gcn.best_params_)\n",
        "                  print(gs_gcn.score(train_graph_embeddings, train_labels))\n",
        "\n",
        "                  if(epoch == args.num_epochs):\n",
        "                    predicted = cls.predict(test_graph_embeddings)\n",
        "                    predicted_labels.append(predicted)\n",
        "\n",
        "                    acc, auc, precision, recall, f1score = calculate_acc(predicted, test_labels)\n",
        "                    acc_all.append(acc)\n",
        "              print(\"Epoch:\", epoch)\n",
        "              mean_10folds = statistics.mean(acc_10folds)\n",
        "              print(\"Test ACC:\", mean_10folds)\n",
        "              mean_trainacc_10folds = statistics.mean(trainacc_10folds)\n",
        "              print(\"Train ACC:\", mean_trainacc_10folds)\n",
        "\n",
        "              acc_test_all.append(mean_10folds)\n",
        "              acc_train_all.append(mean_trainacc_10folds)\n",
        "              \n",
        "\n",
        "              # ACC_test = cls.score(test_graph_embeddings, test_labels)\n",
        "              # acc_test_all.append(ACC_test)\n",
        "\n",
        "              mean_each_epoch.append(str(mean_10folds*100))\n",
        "\n",
        "  return acc_test_all, acc_train_all\n",
        "\n",
        "# all_datasets = [\"dataset1\",\"dataset2\",\"dataset3\",\"dataset4\",\"dataset5\"]\n",
        "all_datasets = [\"dataset1\"]\n",
        "all_test_acc = []\n",
        "all_train_acc = []\n",
        "iter = 1\n",
        "for d in range(len(all_datasets)):\n",
        "  for i in range(iter):\n",
        "    graphs, num_classes = load_data(all_datasets[d], use_degree_as_tag)\n",
        "    feature_dim_size = graphs[0].node_features.shape[1]\n",
        "    graph_labels = np.array([graph.label for graph in graphs])\n",
        "\n",
        "    graph_pool = get_graphpool(graphs)\n",
        "    # print(\"Graph_pool :\", graph_pool)\n",
        "    batch_nodes = Batch_Loader()\n",
        "\n",
        "    print(\"Loading data... finished!\")\n",
        "    print(all_datasets[d], \" loaded\")\n",
        "\n",
        "    acc_test_all, acc_train_all = run_HBCompare_training()\n",
        "    all_test_acc.append(acc_test_all)\n",
        "    all_train_acc.append(acc_train_all)"
      ],
      "metadata": {
        "id": "A75dzPbndVjh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Learning Curve for HBcompare\n",
        "\n",
        "This function gets the training and testing accuracy of HBcompare and plots the values.\n",
        "\n",
        "Differences between this and the main HBcompare training function:\n",
        "\n",
        "- Adds 50 more epochs to show learning curve after maximum saturation\n",
        "- Adds predictions on training set to get training accuracy\n",
        "- Loops through all datasets in the \"all_datasets\" list\n",
        "- Displays learning curve figures for all datasets"
      ],
      "metadata": {
        "id": "QXZX8bs7V1wz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "np.random.seed(123)\n",
        "tf.compat.v1.set_random_seed(123)\n",
        "\n",
        "import os\n",
        "import time\n",
        "import datetime\n",
        "import pickle as cPickle\n",
        "from argparse import ArgumentParser, ArgumentDefaultsHelpFormatter\n",
        "from scipy.sparse import coo_matrix\n",
        "import statistics\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import learning_curve\n",
        "\n",
        "from sklearn import svm\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import scipy.io\n",
        "from tensorflow.keras import backend as K\n",
        "from keract import get_activations\n",
        "import keract\n",
        "\n",
        "# all_datasets = [\"dataset1\",\"dataset2\",\"dataset3\",\"dataset4\",\"dataset5\"]\n",
        "# all_datasets = [\"dataset4\"]\n",
        "\n",
        "\n",
        "# Training\n",
        "# ==================================================\n",
        "def run_HBCompare_training():\n",
        "  with tf.Graph().as_default():\n",
        "      session_conf = tf.compat.v1.ConfigProto(allow_soft_placement=args.allow_soft_placement, log_device_placement=args.log_device_placement)\n",
        "      session_conf.gpu_options.allow_growth = True\n",
        "      sess = tf.compat.v1.Session(config=session_conf)\n",
        "      with sess.as_default():\n",
        "          global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
        "          print(\"Feature_dim_size = \", feature_dim_size)\n",
        "          print(\"Hidden_size = \", args.hidden_size)\n",
        "          print(\"Graph_pool_shape = \", graph_pool.shape[0], \" ... \", graph_pool.shape[1])\n",
        "          print(\"num_sampled = \", args.num_sampled)\n",
        "\n",
        "          unsup_gcn = GCN_graph_cls(feature_dim_size=feature_dim_size,\n",
        "                        hidden_size=args.hidden_size,\n",
        "                        num_GNN_layers=args.num_GNN_layers,\n",
        "                        vocab_size=graph_pool.shape[1],\n",
        "                        num_sampled=args.num_sampled,\n",
        "                        )\n",
        "\n",
        "          # Define Training procedure\n",
        "          optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=args.learning_rate)\n",
        "          grads_and_vars = optimizer.compute_gradients(unsup_gcn.total_loss)\n",
        "          train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
        "\n",
        "          # Folder to save run logs\n",
        "          out_dir = os.path.abspath(os.path.join(args.run_folder, \"../runs_GCN_UnSup\", args.model_name))\n",
        "          print(\"Writing to {}\\n\".format(out_dir))\n",
        "          # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
        "          checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
        "          checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
        "          if not os.path.exists(checkpoint_dir):\n",
        "              os.makedirs(checkpoint_dir)\n",
        "\n",
        "          # Initialize all variables\n",
        "          sess.run(tf.compat.v1.global_variables_initializer())\n",
        "          graph = tf.compat.v1.get_default_graph()\n",
        "\n",
        "          def train_step(Adj_block, X_concat, num_features_nonzero, idx_nodes):\n",
        "              feed_dict = {\n",
        "                  unsup_gcn.Adj_block: Adj_block,\n",
        "                  unsup_gcn.X_concat: X_concat,\n",
        "                  unsup_gcn.num_features_nonzero: num_features_nonzero,\n",
        "                  unsup_gcn.dropout: args.dropout,\n",
        "                  unsup_gcn.input_y:idx_nodes\n",
        "              }\n",
        "              _, step, loss = sess.run([train_op, global_step, unsup_gcn.total_loss], feed_dict)\n",
        "              return loss\n",
        "\n",
        "          # write_acc = open(checkpoint_prefix + '_acc.txt', 'w')\n",
        "          max_acc = 0.0\n",
        "          idx_epoch = 0\n",
        "          num_batches_per_epoch = int((len(graphs) - 1) / args.batch_size) + 1\n",
        "          predicted_labels = []\n",
        "          acc_all = []\n",
        "          acc_test_all = []\n",
        "          acc_train_all = []\n",
        "\n",
        "          mean_each_epoch = []\n",
        "\n",
        "\n",
        "          start_time = time.time()\n",
        "\n",
        "          # Start epochs from 1-50\n",
        "          for epoch in range(1, args.num_epochs+51):\n",
        "\n",
        "              # feature update using loss function\n",
        "              loss = 0\n",
        "              for _ in range(num_batches_per_epoch):\n",
        "                  Adj_block, X_concat, num_features_nonzero, idx_nodes = batch_nodes()\n",
        "                  loss += train_step(Adj_block, X_concat, num_features_nonzero, idx_nodes)\n",
        "                  # current_step = tf.compat.v1.train.global_step(sess, global_step)\n",
        "              print(loss)\n",
        "              # It will give tensor object\n",
        "              node_embeddings = graph.get_tensor_by_name('embedding/node_embeddings:0')\n",
        "              node_embeddings = sess.run(node_embeddings)\n",
        "              graph_embeddings = graph_pool.dot(node_embeddings)\n",
        "\n",
        "              acc_10folds = []\n",
        "              trainacc_10folds = []\n",
        "\n",
        "              # Do 5 fold cross validation per epoch\n",
        "              for fold_idx in range(0,5):\n",
        "                  # Setting up train/test split for 5fold-CV\n",
        "                  train_idx, test_idx = separate_data_idx(graphs, fold_idx)\n",
        "                  train_graph_embeddings = graph_embeddings[train_idx]\n",
        "                  test_graph_embeddings = graph_embeddings[test_idx]\n",
        "                  train_labels = graph_labels[train_idx]\n",
        "                  test_labels = graph_labels[test_idx]\n",
        "\n",
        "                  # Logistic regression model for training\n",
        "                  cls = LogisticRegression(tol=0.001, max_iter = 2000, C=0.1)\n",
        "                  # cls = svm.SVC()\n",
        "                  trained_model = cls.fit(train_graph_embeddings, train_labels)\n",
        "\n",
        "                  predicted_train = cls.predict(train_graph_embeddings)\n",
        "                  ACC_train = cal_acc_only(predicted_train, train_labels)\n",
        "                  trainacc_10folds.append(ACC_train)\n",
        "\n",
        "                  predicted_test = cls.predict(test_graph_embeddings)\n",
        "                  ACC = cal_acc_only(predicted_test, test_labels)\n",
        "                  acc_10folds.append(ACC)\n",
        "\n",
        "                  if(epoch == args.num_epochs):\n",
        "                    predicted = cls.predict(test_graph_embeddings)\n",
        "                    predicted_labels.append(predicted)\n",
        "\n",
        "                    acc, auc, precision, recall, f1score = calculate_acc(predicted, test_labels)\n",
        "                    acc_all.append(acc)\n",
        "              print(\"Epoch:\", epoch)\n",
        "              mean_10folds = statistics.mean(acc_10folds)\n",
        "              print(\"Test ACC:\", mean_10folds)\n",
        "              mean_trainacc_10folds = statistics.mean(trainacc_10folds)\n",
        "              print(\"Train ACC:\", mean_trainacc_10folds)\n",
        "\n",
        "              acc_test_all.append(mean_10folds)\n",
        "              acc_train_all.append(mean_trainacc_10folds)\n",
        "              \n",
        "\n",
        "              # ACC_test = cls.score(test_graph_embeddings, test_labels)\n",
        "              # acc_test_all.append(ACC_test)\n",
        "\n",
        "              mean_each_epoch.append(str(mean_10folds*100))\n",
        "\n",
        "  return acc_test_all, acc_train_all\n",
        "\n",
        "\n",
        "all_test_acc = []\n",
        "all_train_acc = []\n",
        "iter = 2\n",
        "for d in range(len(all_datasets)):\n",
        "  for i in range(iter):\n",
        "    graphs, num_classes = load_data(all_datasets[d], use_degree_as_tag)\n",
        "    feature_dim_size = graphs[0].node_features.shape[1]\n",
        "    graph_labels = np.array([graph.label for graph in graphs])\n",
        "\n",
        "    graph_pool = get_graphpool(graphs)\n",
        "    # print(\"Graph_pool :\", graph_pool)\n",
        "    batch_nodes = Batch_Loader()\n",
        "\n",
        "    print(\"Loading data... finished!\")\n",
        "    print(all_datasets[d], \" loaded\")\n",
        "\n",
        "    acc_test_all, acc_train_all = run_HBCompare_training()\n",
        "    all_test_acc.append(acc_test_all)\n",
        "    all_train_acc.append(acc_train_all)"
      ],
      "metadata": {
        "id": "JbIPhyOAV1PM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# label = [\"P1\", \"A1\", \"P2\", \"A2\", \"P3\"]\n",
        "# plt.figure(0)\n",
        "fig_test_acc_per_epoch = []\n",
        "fig_train_acc_per_epoch = []\n",
        "\n",
        "temp = 0\n",
        "start = 0\n",
        "inc = iter\n",
        "\n",
        "\n",
        "for idx in range(len(all_datasets)):\n",
        "  test_list = []\n",
        "  train_list = []\n",
        "  for x in range(args.num_epochs + 50):\n",
        "    temp = 0\n",
        "    for i in range(start,start+inc):\n",
        "      temp = temp + float(all_test_acc[i][x])\n",
        "    test_list.append(float(temp/inc))\n",
        "\n",
        "    temp_list = []\n",
        "    temp = 0\n",
        "    for i in range(start,start+inc):\n",
        "      temp = temp + float(all_train_acc[i][x])\n",
        "    train_list.append(float(temp/inc))\n",
        "\n",
        "  fig_test_acc_per_epoch.append(test_list)\n",
        "  fig_train_acc_per_epoch.append(train_list)\n",
        "  start = start + inc\n",
        "\n",
        "print(fig_test_acc_per_epoch)\n",
        "print(fig_train_acc_per_epoch)\n",
        "\n",
        "for idx in range(len(all_datasets)):\n",
        "  plt.figure(idx, dpi=1000, figsize=[10,5])\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.ylabel(\"Accuracy\")\n",
        "\n",
        "  plt.yticks(np.arange(0.0,1.05, 0.1))\n",
        "  plt.ylim(0.5,1.05)\n",
        "\n",
        "  plt.plot(range(100),fig_train_acc_per_epoch[idx], label = \"Training\")\n",
        "  plt.plot(range(100),fig_test_acc_per_epoch[idx], label = \"Testing\")\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "# for i in range(len(all_datasets)):\n",
        "#   plt.figure(i)\n",
        "#   plt.xlabel(\"Epoch\")\n",
        "#   plt.ylabel(\"Accuracy\")\n",
        "#   plt.plot(range(50),all_train_acc[i], label = \"Training\")\n",
        "#   plt.plot(range(50),all_test_acc[i], label = \"Testing\")\n",
        "#   plt.legend()\n",
        "#   plt.show()\n"
      ],
      "metadata": {
        "id": "qbEZwDbZYUsR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "print(rewrite_array(all_test_acc))\n",
        "print(rewrite_array(all_train_acc))\n",
        "\n",
        "def rewrite_array(all_array):\n",
        "  new_array = []\n",
        "  for i in range(len(all_array)):\n",
        "    for j in range(len(all_array[i])):\n",
        "      new_array.append(float(all_array[i][j]))\n",
        "  return new_array\n",
        "\n",
        "csvfile = open('training_vs_testing_all_datasets.csv', 'w', newline='')\n",
        "spamwriter = csv.writer(csvfile, delimiter=',')\n",
        "spamwriter.writerow(rewrite_array(all_test_acc))\n",
        "spamwriter.writerow(rewrite_array(all_train_acc))"
      ],
      "metadata": {
        "id": "2CfBeDYfaSg1"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "remLVL7udJ6n",
        "PmG5G1TgdTak",
        "_FeAr2MvutWR",
        "kUZOCPYKMOaw",
        "EIIaeFoiMg3o",
        "nbpx67cimwyu",
        "Y2wZgmLIxHmg",
        "5TvXqgHQW7Zd",
        "45rV7-v8dVSI"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}