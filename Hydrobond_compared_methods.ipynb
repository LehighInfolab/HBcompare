{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MThirJJyZbk5"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!pwd\n",
        "os.chdir('gdrive/My Drive/Graph_based_methods/Graph_Transformer/')\n",
        "!pwd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wcxO_Hxpyhb"
      },
      "source": [
        "### Util"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dCquw44_p0Uj"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import random\n",
        "import scipy.sparse as sp\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from scipy.sparse import isspmatrix\n",
        "\n",
        "\"\"\"Adapted from https://github.com/weihua916/powerful-gnns/blob/master/util.py\"\"\"\n",
        "\n",
        "class S2VGraph(object):\n",
        "    def __init__(self, g, label, node_tags=None, node_features=None):\n",
        "        '''\n",
        "            g: a networkx graph\n",
        "            label: an integer graph label\n",
        "            node_tags: a list of integer node tags\n",
        "            node_features: a torch float tensor, one-hot representation of the tag that is used as input to neural nets\n",
        "            edge_mat: a torch long tensor, contain edge list, will be used to create torch sparse tensor\n",
        "            neighbors: list of neighbors (without self-loop)\n",
        "        '''\n",
        "        self.label = label\n",
        "        self.g = g\n",
        "        self.node_tags = node_tags\n",
        "        self.neighbors = []\n",
        "        self.node_features = 0\n",
        "        self.edge_mat = 0\n",
        "        self.max_neighbor = 0\n",
        "\n",
        "\n",
        "def load_data(dataset, degree_as_tag):\n",
        "    '''\n",
        "        dataset: name of dataset\n",
        "        test_proportion: ratio of test train split\n",
        "        seed: random seed for random splitting of dataset\n",
        "    '''\n",
        "\n",
        "    print('loading data')\n",
        "    g_list = []\n",
        "    label_dict = {}\n",
        "    feat_dict = {}\n",
        "\n",
        "    with open('/content/gdrive/My Drive/Graph_based_methods/Graph_Transformer/dataset/%s/%s.txt' % (dataset, dataset), 'r') as f:\n",
        "        n_g = int(f.readline().strip())\n",
        "        for i in range(n_g):\n",
        "            row = f.readline().strip().split()\n",
        "            n, l = [int(w) for w in row]\n",
        "            if not l in label_dict:\n",
        "                mapped = len(label_dict)\n",
        "                label_dict[l] = mapped\n",
        "            g = nx.Graph()\n",
        "            node_tags = []\n",
        "            node_features = []\n",
        "            n_edges = 0\n",
        "            for j in range(n):\n",
        "                g.add_node(j)\n",
        "                row = f.readline().strip().split()\n",
        "                tmp = int(row[1]) + 2\n",
        "                if tmp == len(row):\n",
        "                    # no node attributes\n",
        "                    row = [int(w) for w in row]\n",
        "                    attr = None\n",
        "                else:\n",
        "                    row, attr = [int(w) for w in row[:tmp]], np.array([float(w) for w in row[tmp:]])\n",
        "                if not row[0] in feat_dict:\n",
        "                    mapped = len(feat_dict)\n",
        "                    feat_dict[row[0]] = mapped\n",
        "                node_tags.append(feat_dict[row[0]])\n",
        "\n",
        "                if tmp > len(row):\n",
        "                    node_features.append(attr)\n",
        "\n",
        "                n_edges += row[1]\n",
        "                for k in range(2, len(row)):\n",
        "                    g.add_edge(j, row[k])\n",
        "\n",
        "            if node_features != []:\n",
        "                node_features = np.stack(node_features)\n",
        "                node_feature_flag = True\n",
        "            else:\n",
        "                node_features = None\n",
        "                node_feature_flag = False\n",
        "\n",
        "            assert len(g) == n\n",
        "\n",
        "            g_list.append(S2VGraph(g, l, node_tags))\n",
        "     \n",
        "\n",
        "    #add labels and edge_mat       \n",
        "    for g in g_list:\n",
        "        g.neighbors = [[] for i in range(len(g.g))]\n",
        "        for i, j in g.g.edges():\n",
        "            g.neighbors[i].append(j)\n",
        "            g.neighbors[j].append(i)\n",
        "        degree_list = []\n",
        "        for i in range(len(g.g)):\n",
        "            g.neighbors[i] = g.neighbors[i]\n",
        "            degree_list.append(len(g.neighbors[i]))\n",
        "        g.max_neighbor = max(degree_list)\n",
        "\n",
        "        g.label = label_dict[g.label]\n",
        "\n",
        "        edges = [list(pair) for pair in g.g.edges()]\n",
        "        edges.extend([[i, j] for j, i in edges])\n",
        "\n",
        "        deg_list = list(dict(g.g.degree(range(len(g.g)))).values())\n",
        "\n",
        "        g.edge_mat = np.transpose(np.array(edges, dtype=np.int32), (1,0))\n",
        "\n",
        "    if degree_as_tag:\n",
        "        for g in g_list:\n",
        "            g.node_tags = list(dict(g.g.degree).values())\n",
        "\n",
        "    #Extracting unique tag labels   \n",
        "    tagset = set([])\n",
        "    for g in g_list:\n",
        "        tagset = tagset.union(set(g.node_tags))\n",
        "\n",
        "    tagset = list(tagset)\n",
        "    tag2index = {tagset[i]:i for i in range(len(tagset))}\n",
        "\n",
        "    for g in g_list:\n",
        "        g.node_features = np.zeros((len(g.node_tags), len(tagset)), dtype=np.float32)\n",
        "        g.node_features[range(len(g.node_tags)), [tag2index[tag] for tag in g.node_tags]] = 1\n",
        "\n",
        "\n",
        "    print('# classes: %d' % len(label_dict))\n",
        "    print('# maximum node tag: %d' % len(tagset))\n",
        "\n",
        "    print(\"# data: %d\" % len(g_list))\n",
        "\n",
        "    return g_list, len(label_dict)\n",
        "\n",
        "\n",
        "\"\"\"Convert sparse matrix to tuple representation.\"\"\"\n",
        "def sparse_to_tuple(sparse_mx):\n",
        "    def to_tuple(mx):\n",
        "        if not sp.isspmatrix_coo(mx):\n",
        "            mx = mx.tocoo()\n",
        "        coords = np.vstack((mx.row, mx.col)).transpose()\n",
        "        values = mx.data\n",
        "        shape = mx.shape\n",
        "        return coords, values, shape\n",
        "\n",
        "    if isinstance(sparse_mx, list):\n",
        "        for i in range(len(sparse_mx)):\n",
        "            sparse_mx[i] = to_tuple(sparse_mx[i])\n",
        "    else:\n",
        "        sparse_mx = to_tuple(sparse_mx)\n",
        "\n",
        "    return sparse_mx\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set up dataset data"
      ],
      "metadata": {
        "id": "PuLn2om5T1gk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_with_ = \"dataset_2\"\n",
        "data_without_ = \"dataset2\"\n",
        "data_name = data_without_\n",
        "use_degree_as_tag = False\n",
        "\n",
        "graphs, num_classes = load_data(data_name, use_degree_as_tag)\n",
        "\n",
        "os.chdir('/content/gdrive/My Drive/Graph_based_methods/Graph_Transformer/dataset/Features_Omar_new')\n",
        "\n",
        "dataset = data_with_\n",
        "# dataset_name = dataset + '_RBF_plus_connectivity_manual_matrix.mat'\n",
        "# dataset_name = dataset + '_RBF_manual_matrix.mat'\n",
        "# dataset_name = dataset + '_label_embedding_matrix.mat'\n",
        "dataset_name = dataset + '_connectivity_matrix.mat'"
      ],
      "metadata": {
        "id": "z2AJO_qATxnv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y45Q0HJanW8L"
      },
      "source": [
        "### KNN Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qan0fFSYnX_6"
      },
      "outputs": [],
      "source": [
        "def Knn_classifier(X, label_all, train_idx, test_idx):\n",
        "  X_train = X[train_idx,:]\n",
        "  X_test = X[test_idx,:]\n",
        "  train_label = label_all[train_idx]\n",
        "  test_label = label_all[test_idx]\n",
        "\n",
        "  num_test = len(list(test_idx))\n",
        "  num_train = len(list(train_idx))\n",
        "\n",
        "  predict_labels = []\n",
        "\n",
        "  for i in range(num_test):\n",
        "    X_test_i = X_test[i,:]\n",
        "    dist_i = []\n",
        "    for j in range(num_train):\n",
        "      X_train_j = X_train[j,:]\n",
        "      dist_ij = np.linalg.norm(X_test_i - X_train_j)\n",
        "      dist_i.append(dist_ij)\n",
        "\n",
        "    min_idx = np.argmin(np.array(dist_i))\n",
        "    label_i = train_label[min_idx]\n",
        "    predict_labels.append(label_i)\n",
        "\n",
        "  return predict_labels \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHd7IYzkkdsm"
      },
      "source": [
        "### Visualize Feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJz5_e95uwYZ"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn import datasets\n",
        "import matplotlib.pylab as plt\n",
        "import scipy.sparse as sparse\n",
        "\n",
        "def visualize_with_tSNE(X, labels):\n",
        "  classes = np.unique(labels)\n",
        "  num_classes = classes.size \n",
        "  y = labels\n",
        "\n",
        "  tsne = TSNE(n_components=2, random_state=0)\n",
        "\n",
        "  X_2d = tsne.fit_transform(X)\n",
        "\n",
        "  target_ids = range(num_classes)\n",
        "  label_name = np.array(range(num_classes))\n",
        "\n",
        "\n",
        "  plt.figure(figsize=(6, 5))\n",
        "  colors = 'r', 'g', 'b', 'c', 'm', 'y', 'k', 'w', 'orange', 'purple'\n",
        "  for i, c, label in zip(target_ids, colors, label_name):\n",
        "      if(i == 0):\n",
        "        marker_label = '^'\n",
        "        str_label = 'Class: 0'\n",
        "      if(i == 1):\n",
        "        marker_label = 'o'  \n",
        "        str_label = 'Class: 1'\n",
        "      if(i == 2):\n",
        "        marker_label = '+'  \n",
        "        str_label = 'Class: 2'\n",
        "      if(i == 3):\n",
        "        marker_label = '*'  \n",
        "        str_label = 'Class: 3'      \n",
        "      plt.scatter(X_2d[y == i, 0], X_2d[y == i, 1], marker = marker_label ,c=c, label= str(str_label))\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "# # create a sparse diagonal matrix with ones on the diagonal\n",
        "# A = sparse.eye(100)\n",
        "# # visualize the sparse matrix with Spy\n",
        "# plt.spy(A, markersize=2)\n",
        "\n",
        "# num_graphs,_,_ = X_feature.shape\n",
        "\n",
        "# for i in range(num_graphs):\n",
        "#   X_feature_i = X_feature[10,:,:]\n",
        "#   plt.spy(X_feature_i, markersize=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTizzhHQLLi0"
      },
      "source": [
        "### FCN Classify"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VkPOeGPHLM80"
      },
      "outputs": [],
      "source": [
        "!pip install keract\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import scipy.io\n",
        "from keras import backend as K\n",
        "from keract import get_activations\n",
        "import keract\n",
        "\n",
        "def FCN_classify(X_train, X_test, label_train, label_test, num_classes):\n",
        "  model = models.Sequential()\n",
        "  model.add(layers.Dense(64, activation='softmax'))\n",
        "  model.add(layers.Dense(num_classes))\n",
        "\n",
        "  # model.summary()\n",
        "\n",
        "  model.compile(optimizer='adam',\n",
        "                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                metrics=['accuracy'])\n",
        "  \n",
        "  history = model.fit(X_train, label_train, epochs=19, verbose =0)\n",
        "\n",
        "  predictions = model.predict(X_test)\n",
        "\n",
        "  predicted_labales = np.argmax(predictions, axis = 1)\n",
        "\n",
        "  # print(' acc ', acc, ' auc ', auc) \n",
        "  return predicted_labales\n",
        "\n",
        "  \n",
        "  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWTNBL8QntFV"
      },
      "source": [
        "### Cal_metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z8gJ4l4onumN"
      },
      "outputs": [],
      "source": [
        ", f1_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from scipy.sparse import isspmatrix\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "\n",
        "\"\"\"Get indexes of train and test sets\"\"\"\n",
        "def separate_data_idx(label_all, fold_idx, seed=0):\n",
        "    assert 0 <= fold_idx and fold_idx < 10, \"fold_idx must be from 0 to 9.\"\n",
        "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
        "\n",
        "    labels = label_all\n",
        "    idx_list = []\n",
        "\n",
        "    for idx in skf.split(np.zeros(len(labels)), labels):\n",
        "        idx_list.append(idx)\n",
        "    train_idx, test_idx = idx_list[fold_idx]\n",
        "\n",
        "    return train_idx, test_idx\n",
        "\n",
        "\n",
        "def calculate_acc(predicted, gt):\n",
        "  correct = 0\n",
        "  for i in range(len(predicted)):\n",
        "    if(predicted[i] == gt[i]):\n",
        "      correct += 1\n",
        "\n",
        "  acc = correct/len(predicted)\n",
        "\n",
        "\n",
        "  lb = preprocessing.LabelBinarizer()\n",
        "  lb.fit(gt)\n",
        "\n",
        "  gt_binary = lb.transform(gt)\n",
        "  predicted_binary = lb.transform(predicted)\n",
        "\n",
        "  auc = roc_auc_score(gt_binary, predicted_binary, average = 'macro')\n",
        "  precision, recall, f1score, support = precision_recall_fscore_support(gt_binary, predicted_binary, average = 'macro')\n",
        "\n",
        "  A = classification_report(predicted, gt, digits = 4)\n",
        "  print(A)\n",
        "\n",
        "\n",
        "  return acc, auc, precision, recall, f1score    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5CN2etPIXdR"
      },
      "source": [
        "### Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5E2eH-GEIa43"
      },
      "outputs": [],
      "source": [
        "### PCA\n",
        "import numpy as np\n",
        "\n",
        "def Feature_reduction_PCA(X, R):\n",
        "  mean_X = np.mean(X,axis=0)\n",
        "  X_normalize = X - mean_X\n",
        "  U, S, V_T = np.linalg.svd(X_normalize, full_matrices=False)\n",
        "  V = np.transpose(V_T)\n",
        "  V_truncate = V[:,0:R]\n",
        "  X_Feature = np.matmul(X_normalize,V_truncate)\n",
        "\n",
        "  return X_Feature\n",
        "\n",
        "def Vec2Matrix(X):\n",
        "  num_graphs, M_square = X.shape \n",
        "  M_matrix = int(np.sqrt(M_square))\n",
        "  X_matrix = np.zeros([num_graphs, int(np.sqrt(M_square)), int(np.sqrt(M_square))])\n",
        "  for i in range(num_graphs):\n",
        "    Xi = X[i,:]\n",
        "    Xi_matrix = np.reshape(Xi, [M_matrix, M_matrix])\n",
        "    X_matrix[i,:,:] = Xi_matrix\n",
        "\n",
        "  return X_matrix\n",
        "\n",
        "### 2DPCA\n",
        "def Feature_reduction_2DPCA(X,R):\n",
        "  X_new = X.copy()\n",
        "  N_graphs,_,_ = X.shape\n",
        "  sum_X = 0\n",
        "  for i in range(N_graphs):\n",
        "    sum_X = sum_X + X[i,:,:]\n",
        "\n",
        "  mean_X = sum_X/N_graphs\n",
        "\n",
        "  for i in range(N_graphs):\n",
        "    X_new[i,:,:] = X[i,:,:] - mean_X\n",
        "\n",
        "  sum_X = 0\n",
        "\n",
        "  for i in range(N_graphs):\n",
        "    Xi = X_new[i,:,:]\n",
        "    sum_X = sum_X + np.matmul(Xi, Xi.transpose())\n",
        "\n",
        "  U,S,V_t = np.linalg.svd(sum_X)\n",
        "  U = U[:,0:R]\n",
        "  X_new_feature = np.zeros([N_graphs,R,R])\n",
        "\n",
        "  for i in range(N_graphs):\n",
        "    Xi = X_new[i,:,:]\n",
        "    Xi = np.matmul(U.transpose(),Xi)  \n",
        "    Xi = np.matmul(Xi,U)  \n",
        "    X_new_feature[i,:,:] = Xi\n",
        "\n",
        "\n",
        "  return X_new_feature  \n",
        "\n",
        "def Matrix2Vec(X):\n",
        "  N, M1,M2 = X.shape\n",
        "  X_vector = np.zeros([N, M1*M2])\n",
        "  for i in range(N):\n",
        "    Xi = X[i,:,:]\n",
        "    Xi_vector = np.reshape(Xi,[1, M1*M2])\n",
        "    X_vector[i,:] = Xi_vector\n",
        "\n",
        "  return X_vector \n",
        "\n",
        "def permute_matrix(X, ratio):\n",
        "  size_X,_ = X.shape \n",
        "  A = np.eye(size_X)\n",
        "  select_num = int(size_X * ratio)\n",
        "  perm_index = np.random.permutation(size_X)   \n",
        "  selected_index = perm_index[0:select_num]\n",
        "  A[selected_index,:] = A[np.random.permutation(selected_index),:];\n",
        "\n",
        "  permuted_X = np.matmul(np.matmul(A,X), A.transpose())\n",
        "\n",
        "  return permuted_X, A\n",
        "\n",
        "def load_and_process_node_features(data_name, use_degree_as_tag):  \n",
        "    graphs, num_classes = load_data(data_name, use_degree_as_tag)\n",
        "    min_num_nodes = 100000\n",
        "    num_graphs = len(graphs)\n",
        "    for i in range(num_graphs):\n",
        "      graph_i = graphs[i]\n",
        "      graph_i_feature = graph_i.node_features\n",
        "      num_nodes,feature_size = graph_i_feature.shape\n",
        "      if(num_nodes < min_num_nodes):\n",
        "        min_num_nodes = num_nodes\n",
        "\n",
        "    feature_all_selected = np.zeros([num_graphs,min_num_nodes*feature_size])  \n",
        "\n",
        "    for i in range(num_graphs):\n",
        "      graph_i = graphs[i]\n",
        "      graph_i_feature = graph_i.node_features.copy()\n",
        "      graph_i_feature_selected = graph_i_feature[0:min_num_nodes,:]\n",
        "      graph_i_feature_vector = np.reshape(graph_i_feature_selected, [1,min_num_nodes*feature_size])\n",
        "      feature_all_selected[i,:] = graph_i_feature_vector  \n",
        "\n",
        "    return feature_all_selected\n",
        "\n",
        "# data_name = \"dataset1\"\n",
        "# use_degree_as_tag = False\n",
        "\n",
        "# feature_all_selected = load_and_process_node_features(data_name, use_degree_as_tag)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mi52pXK6dbzW"
      },
      "source": [
        "###Print Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ObJPKUvGZ6k"
      },
      "outputs": [],
      "source": [
        "from numpy import std\n",
        "def print_results(total_acc, total_auc, total_f1score):\n",
        "  print(\"Accuracy from all runs: \", total_acc)\n",
        "  print(\"AUC from all runs: \",total_auc)\n",
        "  total = 0\n",
        "  count = 0\n",
        "  for i in range(len(total_acc)):\n",
        "    for each in total_acc[i]:\n",
        "      total += each\n",
        "      count += 1\n",
        "  print(\"Average acc = \", total/count)\n",
        "  print(\"STD = \", std(total_acc))\n",
        "\n",
        "  total = 0\n",
        "  count = 0\n",
        "  for i in range(len(total_auc)):\n",
        "    for each in total_auc[i]:\n",
        "      total += each\n",
        "      count += 1\n",
        "  print(\"Average auc = \", total/count)\n",
        "  print(\"STD = \", std(total_auc))\n",
        "\n",
        "  total = 0\n",
        "  count = 0\n",
        "  for i in range(len(total_f1score)):\n",
        "    for each in total_f1score[i]:\n",
        "      total += each\n",
        "      count += 1\n",
        "  print(\"Average f1score = \", total/count)\n",
        "  print(\"STD = \", std(total_f1score))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xg-e8cPJk2Fh"
      },
      "source": [
        "### Graph Kernel Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2KuK6dnWcvo5"
      },
      "outputs": [],
      "source": [
        "!pip install grakel\n",
        "!pip install sklearn\n",
        "\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "from scipy.sparse import spmatrix\n",
        "\n",
        "from warnings import warn\n",
        "from collections import Counter, Iterable\n",
        "from grakel import Kernel, Graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTWAzqOuq3jf"
      },
      "source": [
        "### Graph Kernel Method\n",
        "\n",
        "** This method does not work at this time.\n",
        "Applies grakel methods to a list of adjacency matrix representations of graphs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80_Dpr0Vk1rM"
      },
      "outputs": [],
      "source": [
        "from grakel.kernels.graphlet_sampling import GraphletSampling\n",
        ", train_test_split\n",
        "import numpy as np\n",
        "import scipy.io\n",
        "import scipy.sparse\n",
        "import os\n",
        "from sklearn import svm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from grakel import GraphKernel, ShortestPath, Graph\n",
        "from grakel.kernels import WeisfeilerLehman, VertexHistogram\n",
        "from grakel.datasets import fetch_dataset\n",
        "from grakel.utils import cross_validate_Kfold_SVM\n",
        "\n",
        "# MUTAG = fetch_dataset(\"MUTAG\", verbose=False)\n",
        "# G = MUTAG.data\n",
        "# y = MUTAG.target\n",
        "# print(G)\n",
        "# print(y)\n",
        "\n",
        "# wl_kernel = WeisfeilerLehman(n_iter=5, normalize=True, base_graph_kernel=VertexHistogram)\n",
        "# G_train, G_test, y_train, y_test = train_test_split(G, y, test_size=0.1, random_state=42)\n",
        "\n",
        "# print(G_train)\n",
        "\n",
        "os.chdir('/content/gdrive/My Drive/Graph_based_methods/Graph_Transformer/dataset/Features_Omar_new')\n",
        "\n",
        "data_with_ = \"dataset_3\"\n",
        "data_without_ = \"dataset3\"\n",
        "\n",
        "dataset = data_with_\n",
        "data_name = data_without_\n",
        "use_degree_as_tag = False\n",
        "# dataset_name = dataset + '_RBF_plus_connectivity_manual_matrix.mat'\n",
        "# dataset_name = dataset + '_RBF_manual_matrix.mat'\n",
        "# dataset_name = dataset + '_label_embedding_matrix.mat'\n",
        "dataset_name = dataset + '_connectivity_matrix.mat'\n",
        "\n",
        "mat = scipy.io.loadmat(dataset_name)\n",
        "X = mat['X_feature_tensor']\n",
        "ndims_X = np.ndim(X)\n",
        "\n",
        "# if(ndims_X == 3):\n",
        "#   X = Matrix2Vec(X)\n",
        "\n",
        "# X_feature = X\n",
        "\n",
        "label_name = 'label_'+ dataset + '.mat'\n",
        "mat = scipy.io.loadmat(label_name)\n",
        "label_all = mat['label_all'][0]\n",
        "classes = np.unique(label_all)\n",
        "num_classes = classes.size\n",
        "\n",
        "print(\"Labels\")\n",
        "print(label_all)\n",
        "print(\"Example matrix in X_feature\")\n",
        "print(X[5])\n",
        "\n",
        "all_graphs = []\n",
        "for i in X:\n",
        "  sparse_mat = sparse.csr_matrix(i)\n",
        "  graph = Graph(sparse_mat, node_labels = label_all)\n",
        "  all_graphs.append(graph)\n",
        "\n",
        "def run_gk():\n",
        "  # visualize_with_tSNE(X_feature, label_all)\n",
        "\n",
        "  print('*********** dataset_name =', dataset_name,' *******************')\n",
        "\n",
        "  predicted_labels = []\n",
        "  acc_all = []\n",
        "  auc_all = []\n",
        "  f1score_all = []\n",
        "\n",
        "  ##**** Logistic Regression **** #####\n",
        "  # for fold_idx in range(5):\n",
        "      # train_idx, test_idx = separate_data_idx(label_all, fold_idx)\n",
        "      # train_embeddings = X_feature[train_idx,:]\n",
        "      # test_embeddings = X_feature[test_idx,:]\n",
        "      # train_labels = label_all[train_idx]\n",
        "      # test_labels = label_all[test_idx]\n",
        "\n",
        "  G_train, G_test, y_train, y_test = train_test_split(all_graphs, label_all, test_size=0.1)\n",
        "\n",
        "  #GK transform\n",
        "  # gk = WeisfeilerLehman(base_graph_kernel = VertexHistogram, normalize=True)\n",
        "  # gk = ShortestPath(normalize=True, with_labels=False)\n",
        "  # gk = GraphletSampling(normalize=True)\n",
        "\n",
        "  # for i in G_train:\n",
        "  # K_train = gk.fit_transform(G_train)\n",
        "  # K_test = gk.transform(G_test)\n",
        "\n",
        "  # cls = LogisticRegression(tol=0.001, max_iter = 2000)\n",
        "  # cls.fit(train_embeddings, train_labels)\n",
        "  cls = svm.SVC(kernel=\"linear\")\n",
        "  cls.fit(G_train, y_train)\n",
        "  ACC = cls.score(G_test, y_test)\n",
        "\n",
        "  predicted = cls.predict(G_test)\n",
        "  print(predicted)\n",
        "\n",
        "  acc, auc, precision, recall, f1score = calculate_acc(predicted, y_test)\n",
        "\n",
        "  print(' acc ', acc, ' auc ', auc, ' precision ', precision, ' recall ', recall, ' f1score ', f1score) \n",
        "\n",
        "  \n",
        "  acc_all.append(acc)\n",
        "  auc_all.append(auc)\n",
        "  f1score_all.append(f1score)\n",
        "\n",
        "  acc_all = np.array(acc_all)\n",
        "  auc_all = np.array(auc_all)\n",
        "  f1score_all = np.array(f1score_all)\n",
        "\n",
        "  # print('acc_all = ', acc_all)   \n",
        "\n",
        "  # print('auc_all = ', auc_all) \n",
        "  \n",
        "  return acc_all, auc_all, f1score_all\n",
        "\n",
        "total_acc = []\n",
        "total_auc = []\n",
        "total_f1score = []\n",
        "iter = 1\n",
        "for i in range(iter):\n",
        "  # run_gk()\n",
        "  acc_all,auc_all, f1score_all = run_gk()\n",
        "  total_acc.append(acc_all)\n",
        "  total_auc.append(auc_all)\n",
        "  total_f1score.append(f1score_all)\n",
        "\n",
        "# print_results(total_acc, total_auc, total_f1score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XVYx2d8JTe6_"
      },
      "outputs": [],
      "source": [
        "print(\"ACC:\", np.mean(total_acc))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GK Linear Kernel\n",
        "\n",
        "This method applies a linear kernel to the adjacency matrix representing the overall graph."
      ],
      "metadata": {
        "id": "qD2eFV1UnYx1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy.io\n",
        "import os\n",
        "from sklearn import svm\n",
        "\n",
        "os.chdir('/content/gdrive/My Drive/Graph_based_methods/Graph_Transformer/dataset/Features_Omar_new')\n",
        "\n",
        "data_with_ = \"dataset_5\"\n",
        "data_without_ = \"dataset5\"\n",
        "dataset = data_with_\n",
        "data_name = data_without_\n",
        "use_degree_as_tag = False\n",
        "# dataset_name = dataset + '_RBF_plus_connectivity_manual_matrix.mat'\n",
        "# dataset_name = dataset + '_RBF_manual_matrix.mat'\n",
        "# dataset_name = dataset + '_label_embedding_matrix.mat'\n",
        "dataset_name = dataset + '_connectivity_matrix.mat'\n",
        "\n",
        "def run_PCA():\n",
        "  # random.seed()\n",
        "  mat = scipy.io.loadmat(dataset_name)\n",
        "  X = mat['X_feature_tensor']\n",
        "\n",
        "  ndims_X = np.ndim(X)\n",
        "\n",
        "  if(ndims_X == 3):\n",
        "    X = Matrix2Vec(X)\n",
        "\n",
        "  X_feature = X\n",
        "\n",
        "  label_name = 'label_'+ dataset + '.mat'\n",
        "  mat = scipy.io.loadmat(label_name)\n",
        "  label_all = mat['label_all'][0]\n",
        "  classes = np.unique(label_all)\n",
        "  num_classes = classes.size\n",
        "\n",
        "  # visualize_with_tSNE(X_feature, label_all)\n",
        "\n",
        "  print('*********** dataset_name =', dataset_name,' *******************')\n",
        "\n",
        "  predicted_labels = []\n",
        "  acc_all = []\n",
        "  auc_all = []\n",
        "  f1score_all = []\n",
        "\n",
        "\n",
        "  ##**** Logistic Regression **** #####\n",
        "  for fold_idx in range(5):\n",
        "      train_idx, test_idx = separate_data_idx(label_all, fold_idx)\n",
        "      train_embeddings = X_feature[train_idx,:]\n",
        "      test_embeddings = X_feature[test_idx,:]\n",
        "      train_labels = label_all[train_idx]\n",
        "      test_labels = label_all[test_idx]\n",
        "\n",
        "      # cls = LogisticRegression(tol=0.001, max_iter = 2000)\n",
        "      cls = svm.SVC(kernel=\"linear\")\n",
        "      cls.fit(train_embeddings, train_labels)\n",
        "      ACC = cls.score(test_embeddings, test_labels)\n",
        "\n",
        "      predicted = cls.predict(test_embeddings)\n",
        "      predicted_labels.append(predicted)\n",
        "\n",
        "      acc, auc, precision, recall, f1score = calculate_acc(predicted, test_labels)\n",
        "\n",
        "      print('fold ', fold_idx, ' acc ', ACC, ' auc ', auc, ' precision ', precision, ' recall ', recall, ' f1score ', f1score) \n",
        "\n",
        "      \n",
        "      acc_all.append(acc)\n",
        "      auc_all.append(auc)\n",
        "      f1score_all.append(f1score)\n",
        "\n",
        "  acc_all = np.array(acc_all)\n",
        "  auc_all = np.array(auc_all)\n",
        "  f1score_all = np.array(f1score_all)\n",
        "\n",
        "  # print('acc_all = ', acc_all)   \n",
        "\n",
        "  # print('auc_all = ', auc_all) \n",
        "  \n",
        "  return acc_all, auc_all, f1score_all\n",
        "\n",
        "total_acc = []\n",
        "total_auc = []\n",
        "total_f1score = []\n",
        "iter = 1\n",
        "for i in range(iter):\n",
        "  acc_all,auc_all, f1score_all = run_PCA()\n",
        "  total_acc.append(acc_all)\n",
        "  total_auc.append(auc_all)\n",
        "  total_f1score.append(f1score_all)\n",
        "\n",
        "print_results(total_acc, total_auc, total_f1score)"
      ],
      "metadata": {
        "id": "WmipNaWhnZDu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GK Kernel Pre-computed\n",
        "\n",
        "**This method does not work at this time.\n",
        "\n",
        "This function takes a pre-computed kernel matrix and inputs into a 5-fold CV with SVM classifier. \n"
      ],
      "metadata": {
        "id": "zWn6veyhwIKV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F2rJoV1eaXyj"
      },
      "outputs": [],
      "source": [
        "from grakel.kernels.graphlet_sampling import GraphletSampling\n",
        ", train_test_split\n",
        "import numpy as np\n",
        "import scipy.io\n",
        "import scipy.sparse\n",
        "import os\n",
        "from sklearn import svm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from grakel import GraphKernel, ShortestPath, Graph\n",
        "from grakel.kernels import WeisfeilerLehman, VertexHistogram\n",
        "from grakel.datasets import fetch_dataset\n",
        "from grakel.utils import cross_validate_Kfold_SVM\n",
        "\n",
        "\n",
        "os.chdir('/content/gdrive/My Drive/Graph_based_methods/Graph_Transformer/dataset/dataset1')\n",
        "kernel_name = dataset + '_SPkernel.mat'\n",
        "kernel_mat = scipy.io.loadmat(kernel_name)\n",
        "kernel = kernel_mat['Kernel']\n",
        "\n",
        "os.chdir('/content/gdrive/My Drive/Graph_based_methods/Graph_Transformer/dataset/Features_Omar_new')\n",
        "\n",
        "data_with_ = \"dataset_1\"\n",
        "data_without_ = \"dataset1\"\n",
        "\n",
        "dataset = data_with_\n",
        "data_name = data_without_\n",
        "use_degree_as_tag = False\n",
        "# dataset_name = dataset + '_RBF_plus_connectivity_manual_matrix.mat'\n",
        "# dataset_name = dataset + '_RBF_manual_matrix.mat'\n",
        "# dataset_name = dataset + '_label_embedding_matrix.mat'\n",
        "dataset_name = dataset + '_connectivity_matrix.mat'\n",
        "\n",
        "mat = scipy.io.loadmat(dataset_name)\n",
        "X = mat['X_feature_tensor']\n",
        "ndims_X = np.ndim(X)\n",
        "\n",
        "if(ndims_X == 3):\n",
        "  X = Matrix2Vec(X)\n",
        "\n",
        "X_feature = X\n",
        "X_feature = kernel\n",
        "\n",
        "label_name = 'label_'+ dataset + '.mat'\n",
        "mat = scipy.io.loadmat(label_name)\n",
        "label_all = mat['label_all'][0]\n",
        "classes = np.unique(label_all)\n",
        "num_classes = classes.size\n",
        "\n",
        "# X_feature = my_kernel(X_feature,label_all)\n",
        "\n",
        "# def my_kernel(X, Y):\n",
        "#         \"\"\"\n",
        "#         We create a custom kernel:\n",
        "\n",
        "#                     (2  0)\n",
        "#         k(X, Y) = X  (    ) Y.T\n",
        "#                     (0  1)\n",
        "#         \"\"\"\n",
        "#         # print(\"X shape\",X.shape)\n",
        "#         # print(\"Kernel shape\", kernel.shape)\n",
        "        \n",
        "#         # s1 = np.dot(X.T,kernel)\n",
        "#         # print(s1.shape)\n",
        "#         # print(\"label shape\", Y.shape)\n",
        "#         # s2 = np.dot(s1, Y)\n",
        "#         s2 = np.dot(X,X.T)\n",
        "#         # print(\"return shape\",s2.shape)\n",
        "#         return s2\n",
        "\n",
        "# X_feature = my_kernel(X_feature, label_all)\n",
        "# print(\"X_feature final shape\",X_feature.shape)\n",
        "\n",
        "def run_gk():\n",
        "  # visualize_with_tSNE(X_feature, label_all)\n",
        "\n",
        "  print('*********** dataset_name =', dataset_name,' *******************')\n",
        "\n",
        "  predicted_labels = []\n",
        "  acc_all = []\n",
        "  auc_all = []\n",
        "  f1score_all = []\n",
        "\n",
        "  ##**** Logistic Regression **** #####\n",
        "  for fold_idx in range(5):\n",
        "      train_idx, test_idx = separate_data_idx(label_all, fold_idx)\n",
        "      G_train = X_feature[train_idx,:]\n",
        "      G_test = X_feature[test_idx,:]\n",
        "      y_train = label_all[train_idx]\n",
        "      y_test = label_all[test_idx]\n",
        "\n",
        "      # G_train, G_test, y_train, y_test = train_test_split(X_feature, label_all, test_size=0.1)\n",
        "\n",
        "      #GK transform\n",
        "      # K_train = gk.fit_transform(G_train)\n",
        "      # K_test = gk.transform(G_test)\n",
        "\n",
        "      # cls = LogisticRegression(tol=0.001, max_iter = 2000)\n",
        "      cls = svm.SVC(kernel=kernel)\n",
        "      print(\"training...\")\n",
        "      cls.fit(G_train, y_train)\n",
        "      print(\"done.\")\n",
        "      \n",
        "      # cls.fit(K_train, y_train)\n",
        "      # ACC = cls.score(test_embeddings, test_labels)\n",
        "\n",
        "      predicted = cls.predict(G_test)\n",
        "      print(predicted)\n",
        "\n",
        "      acc, auc, precision, recall, f1score = calculate_acc(predicted, y_test)\n",
        "\n",
        "      print(' acc ', acc, ' auc ', auc, ' precision ', precision, ' recall ', recall, ' f1score ', f1score) \n",
        "\n",
        "      \n",
        "      acc_all.append(acc)\n",
        "      auc_all.append(auc)\n",
        "      f1score_all.append(f1score)\n",
        "\n",
        "      acc_all = np.array(acc_all)\n",
        "      auc_all = np.array(auc_all)\n",
        "      f1score_all = np.array(f1score_all)\n",
        "\n",
        "  # print('acc_all = ', acc_all)   \n",
        "\n",
        "  # print('auc_all = ', auc_all) \n",
        "  \n",
        "  return acc_all, auc_all, f1score_all\n",
        "\n",
        "total_acc = []\n",
        "total_auc = []\n",
        "total_f1score = []\n",
        "iter = 1\n",
        "for i in range(iter):\n",
        "  # run_gk()\n",
        "  acc_all,auc_all, f1score_all = run_gk()\n",
        "  total_acc.append(acc_all)\n",
        "  total_auc.append(auc_all)\n",
        "  total_f1score.append(f1score_all)\n",
        "\n",
        "# print_results(total_acc, total_auc, total_f1score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tbwRmOnmh4_I"
      },
      "outputs": [],
      "source": [
        "print(\"ACC:\", np.mean(total_acc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mciqHfHChq7i"
      },
      "source": [
        "### PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7dJizfIIhqPf"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import scipy.io\n",
        "import os\n",
        "\n",
        "os.chdir('/content/gdrive/My Drive/Graph_based_methods/Graph_Transformer/dataset/Features_Omar_new')\n",
        "\n",
        "# dataset = data_with_\n",
        "# data_name = data_without_\n",
        "# use_degree_as_tag = False\n",
        "# dataset_name = dataset + '_RBF_plus_connectivity_manual_matrix.mat'\n",
        "# dataset_name = dataset + '_RBF_manual_matrix.mat'\n",
        "# dataset_name = dataset + '_label_embedding_matrix.mat'\n",
        "# dataset_name = dataset + '_connectivity_matrix.mat'\n",
        "\n",
        "def run_PCA():\n",
        "  # random.seed()\n",
        "  mat = scipy.io.loadmat(dataset_name)\n",
        "  X = mat['X_feature_tensor']\n",
        "\n",
        "  ndims_X = np.ndim(X)\n",
        "\n",
        "  if(ndims_X == 3):\n",
        "    X = Matrix2Vec(X)\n",
        "\n",
        "  X_feature = Feature_reduction_PCA(X, 60)\n",
        "  print(X_feature.shape)\n",
        "  # X_feature = X\n",
        "\n",
        "  label_name = 'label_'+ dataset + '.mat'\n",
        "  mat = scipy.io.loadmat(label_name)\n",
        "  label_all = mat['label_all'][0]\n",
        "  classes = np.unique(label_all)\n",
        "  num_classes = classes.size\n",
        "\n",
        "  # visualize_with_tSNE(X_feature, label_all)\n",
        "\n",
        "  print('*********** dataset_name =', dataset_name,' *******************')\n",
        "\n",
        "  predicted_labels = []\n",
        "  acc_all = []\n",
        "  auc_all = []\n",
        "  f1score_all = []\n",
        "\n",
        "\n",
        "  ##**** Logistic Regression **** #####\n",
        "  for fold_idx in range(5):\n",
        "      train_idx, test_idx = separate_data_idx(label_all, fold_idx)\n",
        "      train_embeddings = X_feature[train_idx,:]\n",
        "      test_embeddings = X_feature[test_idx,:]\n",
        "      train_labels = label_all[train_idx]\n",
        "      test_labels = label_all[test_idx]\n",
        "\n",
        "      cls = LogisticRegression(tol=0.001, max_iter = 2000)\n",
        "      cls.fit(train_embeddings, train_labels)\n",
        "      ACC = cls.score(test_embeddings, test_labels)\n",
        "\n",
        "      predicted = cls.predict(test_embeddings)\n",
        "      predicted_labels.append(predicted)\n",
        "\n",
        "      acc, auc, precision, recall, f1score = calculate_acc(predicted, test_labels)\n",
        "\n",
        "      print('fold ', fold_idx, ' acc ', ACC, ' auc ', auc, ' precision ', precision, ' recall ', recall, ' f1score ', f1score) \n",
        "\n",
        "      \n",
        "      acc_all.append(acc)\n",
        "      auc_all.append(auc)\n",
        "      f1score_all.append(f1score)\n",
        "\n",
        "  acc_all = np.array(acc_all)\n",
        "  auc_all = np.array(auc_all)\n",
        "  f1score_all = np.array(f1score_all)\n",
        "\n",
        "  # print('acc_all = ', acc_all)   \n",
        "\n",
        "  # print('auc_all = ', auc_all) \n",
        "  \n",
        "  return acc_all, auc_all, f1score_all\n",
        "\n",
        "total_acc = []\n",
        "total_auc = []\n",
        "total_f1score = []\n",
        "iter = 1\n",
        "for i in range(iter):\n",
        "  acc_all,auc_all, f1score_all = run_PCA()\n",
        "  total_acc.append(acc_all)\n",
        "  total_auc.append(auc_all)\n",
        "  total_f1score.append(f1score_all)\n",
        "\n",
        "print_results(total_acc, total_auc, total_f1score)\n",
        "##**** FCN Classify **** #####\n",
        "# for fold_idx in range(5):\n",
        "#     train_idx, test_idx = separate_data_idx(label_all, fold_idx)\n",
        "#     train_embeddings = X_feature[train_idx,:]\n",
        "#     test_embeddings = X_feature[test_idx,:]\n",
        "#     train_labels = label_all[train_idx]\n",
        "#     test_labels = label_all[test_idx] \n",
        "\n",
        "#     predicted = FCN_classify(train_embeddings, test_embeddings, train_labels, test_labels, num_classes)\n",
        "#     predicted_labels.append(predicted)\n",
        "\n",
        "#     acc, auc = calculate_acc(predicted, test_labels)\n",
        "\n",
        "#     print('fold ', fold_idx, ' acc ', acc, ' auc ', auc) \n",
        "\n",
        "#     auc_all.append(auc)\n",
        "#     acc_all.append(acc)\n",
        "\n",
        "# acc_all = np.array(acc_all)\n",
        "# auc_all = np.array(auc_all)\n",
        "\n",
        "# print('acc_all = ', acc_all)   \n",
        "\n",
        "# print('auc_all = ', auc_all) \n",
        "\n",
        "\n",
        "##**** Repeat FCN Classify **** #####\n",
        "# iter = 20\n",
        "# acc_all = []\n",
        "# for i in range(iter):\n",
        "#   acc_i = 0\n",
        "#   for fold_idx in range(5):\n",
        "#       train_idx, test_idx = separate_data_idx(label_all, fold_idx)\n",
        "#       train_embeddings = X_feature[train_idx,:]\n",
        "#       test_embeddings = X_feature[test_idx,:]\n",
        "#       train_labels = label_all[train_idx]\n",
        "#       test_labels = label_all[test_idx] \n",
        "\n",
        "#       predicted = FCN_classify(train_embeddings, test_embeddings, train_labels, test_labels, num_classes)\n",
        "#       predicted_labels.append(predicted)\n",
        "\n",
        "#       acc, auc = calculate_acc(predicted, test_labels)\n",
        "\n",
        "#       # print('fold ', fold_idx, ' acc ', acc, ' auc ', auc) \n",
        "\n",
        "#       acc_i += acc\n",
        "\n",
        "#   avg_acc_i = acc_i/(fold_idx + 1) \n",
        "#   acc_all.append(avg_acc_i)   \n",
        "\n",
        "\n",
        "\n",
        "# acc_all = np.array(acc_all)\n",
        "# # auc_all = np.array(auc_all)\n",
        "\n",
        "# print('acc_all = ', acc_all)   \n",
        "\n",
        "# # print('auc_all = ', auc_all) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVkNYNQYojjM"
      },
      "source": [
        "### PCA + node features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8b9XhFNoonGw"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import scipy.io\n",
        "import os\n",
        "\n",
        "# dataset = data_with_\n",
        "# data_name = data_without_\n",
        "# use_degree_as_tag = False\n",
        "# dataset_name = dataset + '_RBF_plus_connectivity_manual_matrix.mat'\n",
        "# dataset_name = dataset + '_RBF_manual_matrix.mat'\n",
        "# dataset_name = dataset + '_label_embedding_matrix.mat'\n",
        "# dataset_name = dataset + '_connectivity_matrix.mat'\n",
        "\n",
        "def run_PCA_NF():\n",
        "  mat = scipy.io.loadmat(dataset_name)\n",
        "  X = mat['X_feature_tensor']\n",
        "\n",
        "  ndims_X = np.ndim(X)\n",
        "\n",
        "  if(ndims_X == 3):\n",
        "    X = Matrix2Vec(X)\n",
        "\n",
        "  X_feature = Feature_reduction_PCA(X, 60)\n",
        "  X_node_feature = load_and_process_node_features(data_name, use_degree_as_tag)\n",
        "\n",
        "  print(X_feature.shape)\n",
        "  print(X_node_feature.shape)\n",
        "\n",
        "  X_feature = np.concatenate((X_feature, X_node_feature), axis = 1)\n",
        "\n",
        "  print(X_feature.shape)\n",
        "\n",
        "  label_name = 'label_'+ dataset + '.mat'\n",
        "  mat = scipy.io.loadmat(label_name)\n",
        "  label_all = mat['label_all'][0]\n",
        "  classes = np.unique(label_all)\n",
        "  num_classes = classes.size\n",
        "\n",
        "  # visualize_with_tSNE(X_feature, label_all)\n",
        "\n",
        "  print('*********** dataset_name =', dataset_name,' *******************')\n",
        "\n",
        "  predicted_labels = []\n",
        "  acc_all = []\n",
        "  auc_all = []\n",
        "  f1score_all = []\n",
        "\n",
        "\n",
        "  ##**** Logistic Regression **** #####\n",
        "  for fold_idx in range(5):\n",
        "      train_idx, test_idx = separate_data_idx(label_all, fold_idx)\n",
        "      print(test_idx)\n",
        "      train_embeddings = X_feature[train_idx,:]\n",
        "      test_embeddings = X_feature[test_idx,:]\n",
        "      train_labels = label_all[train_idx]\n",
        "      test_labels = label_all[test_idx]\n",
        "\n",
        "      cls = LogisticRegression(tol=0.001, max_iter = 2000)\n",
        "      cls.fit(train_embeddings, train_labels)\n",
        "      ACC = cls.score(test_embeddings, test_labels)\n",
        "\n",
        "      predicted = cls.predict(test_embeddings)\n",
        "      predicted_labels.append(predicted)\n",
        "\n",
        "      acc, auc, precision, recall, f1score = calculate_acc(predicted, test_labels)\n",
        "\n",
        "      print('fold ', fold_idx, ' acc ', ACC, ' auc ', auc, ' precision ', precision, ' recall ', recall, ' f1score ', f1score) \n",
        "\n",
        "      \n",
        "      acc_all.append(acc)\n",
        "      auc_all.append(auc)\n",
        "      f1score_all.append(f1score)\n",
        "\n",
        "  acc_all = np.array(acc_all)\n",
        "  auc_all = np.array(auc_all)\n",
        "\n",
        "  # print('acc_all = ', acc_all)   \n",
        "\n",
        "  # print('auc_all = ', auc_all) \n",
        "  return acc_all, auc_all, f1score_all\n",
        "\n",
        "\n",
        "total_acc = []\n",
        "total_auc = []\n",
        "total_f1score = []\n",
        "iter = 10\n",
        "for i in range(iter):\n",
        "  acc_all, auc_all, f1score_all = run_PCA_NF()\n",
        "  total_acc.append(acc_all)\n",
        "  total_auc.append(auc_all)\n",
        "  total_f1score.append(f1score_all)\n",
        "\n",
        "print_results(total_acc, total_auc, total_f1score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAQPJZaAVQbr"
      },
      "source": [
        "### 2DPCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e3DZaM0JcuNZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import scipy.io\n",
        "import os\n",
        "\n",
        "# os.chdir('/content/gdrive/My Drive/Graph_based_methods/Graph_Transformer/dataset/Features_Omar_new')\n",
        "\n",
        "# dataset = data_with_\n",
        "# dataset_name = dataset + '_RBF_plus_connectivity_manual_matrix.mat'\n",
        "# dataset_name = dataset + '_RBF_manual_matrix.mat'\n",
        "# dataset_name = dataset + '_connectivity_matrix.mat'\n",
        "\n",
        "def run_2DPCA():\n",
        "  mat = scipy.io.loadmat(dataset_name)\n",
        "  X = mat['X_feature_tensor']\n",
        "\n",
        "  ndims_X = np.ndim(X)\n",
        "  if(ndims_X == 2):\n",
        "    # X = Matrix2Vec(X)\n",
        "    X_feature = Vec2Matrix(X)\n",
        "  X_feature = X;\n",
        "\n",
        "  X_feature_r = Feature_reduction_2DPCA(X_feature, 60)\n",
        "\n",
        "  X_feature = Matrix2Vec(X_feature_r)\n",
        "\n",
        "  label_name = 'label_'+ dataset + '.mat'\n",
        "  mat = scipy.io.loadmat(label_name)\n",
        "  label_all = mat['label_all'][0]\n",
        "  classes = np.unique(label_all)\n",
        "  num_classes = classes.size\n",
        "\n",
        "  print('*********** dataset_name =', dataset_name,' *******************')\n",
        "\n",
        "\n",
        "  #### Logistic Regression #####\n",
        "  predicted_labels = []\n",
        "  acc_all = []\n",
        "  auc_all = []\n",
        "  f1score_all = []\n",
        "\n",
        "\n",
        "  for fold_idx in range(5):\n",
        "      train_idx, test_idx = separate_data_idx(label_all, fold_idx)\n",
        "      train_embeddings = X_feature[train_idx,:]\n",
        "      test_embeddings = X_feature[test_idx,:]\n",
        "      train_labels = label_all[train_idx]\n",
        "      test_labels = label_all[test_idx]\n",
        "\n",
        "      cls = LogisticRegression(tol=0.001, max_iter = 2000)\n",
        "      cls.fit(train_embeddings, train_labels)\n",
        "      ACC = cls.score(test_embeddings, test_labels)\n",
        "\n",
        "      predicted = cls.predict(test_embeddings)\n",
        "      predicted_labels.append(predicted)\n",
        "\n",
        "      acc, auc, precision, recall, f1score = calculate_acc(predicted, test_labels)\n",
        "\n",
        "      print('fold ', fold_idx, ' acc ', ACC, ' auc ', auc, ' precision ', precision, ' recall ', recall, ' f1score ', f1score) \n",
        "\n",
        "      auc_all.append(auc)\n",
        "      acc_all.append(acc)\n",
        "      f1score_all.append(f1score)\n",
        "\n",
        "  acc_all = np.array(acc_all)\n",
        "  auc_all = np.array(auc_all)\n",
        "  f1score_all = np.array(f1score_all)\n",
        "\n",
        "  return acc_all, auc_all, f1score_all\n",
        "\n",
        "total_acc = []\n",
        "total_auc = []\n",
        "total_f1score = []\n",
        "iter = 10\n",
        "for i in range(iter):\n",
        "  acc_all,auc_all, f1score_all = run_2DPCA()\n",
        "  total_acc.append(acc_all)\n",
        "  total_auc.append(auc_all)\n",
        "  total_f1score.append(f1score_all)\n",
        "\n",
        "print_results(total_acc, total_auc, total_f1score)\n",
        "  # print('acc_all = ', acc_all)   \n",
        "\n",
        "  # print('auc_all = ', auc_all) \n",
        "\n",
        "##**** Repeat FCN Classify **** #####\n",
        "# iter = 20\n",
        "# acc_all = []\n",
        "# for i in range(iter):\n",
        "#   acc_i = 0\n",
        "#   for fold_idx in range(5):\n",
        "#       train_idx, test_idx = separate_data_idx(label_all, fold_idx)\n",
        "#       train_embeddings = X_feature[train_idx,:]\n",
        "#       test_embeddings = X_feature[test_idx,:]\n",
        "#       train_labels = label_all[train_idx]\n",
        "#       test_labels = label_all[test_idx] \n",
        "\n",
        "#       predicted = FCN_classify(train_embeddings, test_embeddings, train_labels, test_labels, num_classes)\n",
        "#       predicted_labels.append(predicted)\n",
        "\n",
        "#       acc, auc = calculate_acc(predicted, test_labels)\n",
        "\n",
        "#       # print('fold ', fold_idx, ' acc ', acc, ' auc ', auc) \n",
        "\n",
        "#       acc_i += acc\n",
        "\n",
        "#   avg_acc_i = acc_i/(fold_idx + 1) \n",
        "#   acc_all.append(avg_acc_i)   \n",
        "\n",
        "\n",
        "\n",
        "# acc_all = np.array(acc_all)\n",
        "# # auc_all = np.array(auc_all)\n",
        "\n",
        "# print('acc_all = ', acc_all)   \n",
        "\n",
        "# print('auc_all = ', auc_all) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIjiACTl6iKP"
      },
      "source": [
        "### CNN classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zSECN1MG6hpe"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip install keract\n",
        "\n",
        "%load_ext tensorboard\n",
        "import tensorflow as tf\n",
        "import datetime, os\n",
        "import tensorflow.keras\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import scipy.io\n",
        "from keras import backend as K\n",
        "from keract import get_activations\n",
        "import keract\n",
        "\n",
        "\n",
        "\n",
        "def initialize_model(num_classes):\n",
        "\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Conv2D(6, (6, 6), activation='relu', input_shape = (600,600,1)))\n",
        "    model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "    model.add(layers.Conv2D(6, (6, 6), activation='relu'))\n",
        "    model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "    # model.add(layers.Conv2D(6, (6, 6), activation='relu'))\n",
        "    # model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "    # model.add(layers.Conv2D(6, (6, 6), activation='relu'))\n",
        "    # model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "    # model.add(layers.Conv2D(6, (6, 6), activation='relu'))\n",
        "    # model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "    # model.add(layers.Conv2D(6, (6, 6), activation='relu'))\n",
        "    # model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(64, activation='relu'))\n",
        "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
        "\n",
        "    \n",
        "    return model\n",
        "\n",
        "permute_ratio = 0\n",
        "\n",
        "\n",
        "# os.chdir('/content/gdrive/My Drive/Graph_based_methods/Graph_Transformer/dataset/Features_Omar_new')\n",
        "\n",
        "# dataset = data_with_\n",
        "# dataset_name = dataset + '_RBF_manual_matrix.mat'\n",
        "# dataset_name = dataset + '_RBF_plus_connectivity_manual_matrix.mat'\n",
        "# dataset_name = dataset + '_RBF_based_on_connectivity.mat'\n",
        "# dataset_name = dataset + '_connectivity_matrix.mat'\n",
        "\n",
        "def run_CNN():\n",
        "  mat = scipy.io.loadmat(dataset_name)\n",
        "  X = mat['X_feature_tensor']\n",
        "\n",
        "  ndims_X = np.ndim(X)\n",
        "\n",
        "  if(ndims_X == 2):\n",
        "    X_feature = Vec2Matrix(X)\n",
        "  elif(ndims_X == 3):\n",
        "    X_feature = X\n",
        "\n",
        "  num_graph,W,H = X_feature.shape\n",
        "  A_all = []\n",
        "\n",
        "  # for i in range(num_graph):\n",
        "  #   X_i = X_feature[i,:,:].copy()\n",
        "  #   X_i_permute,A_i = permute_matrix(X_i, permute_ratio)\n",
        "  #   X_feature[i,:,:] = X_i_permute\n",
        "  #   A_all.append(A_i)\n",
        "\n",
        "\n",
        "  label_name = 'label_'+ dataset + '.mat'\n",
        "  mat = scipy.io.loadmat(label_name)\n",
        "  label_all = mat['label_all'][0]\n",
        "\n",
        "  print('*********** dataset_name =', dataset_name,' *******************')\n",
        "  classes = np.unique(label_all)\n",
        "  num_classes = classes.size\n",
        "  print('number of classes: ', num_classes)\n",
        "\n",
        "\n",
        "  predicted_labels = []\n",
        "  acc_all = []\n",
        "  auc_all = []\n",
        "  f1score_all = []\n",
        "\n",
        "  for fold_idx in range(5):    \n",
        "      train_idx, test_idx = separate_data_idx(label_all, fold_idx)\n",
        "      intersect = np.intersect1d(train_idx, test_idx)\n",
        "      print(intersect)\n",
        "\n",
        "      train_embeddings = X_feature[train_idx,:,:]\n",
        "      test_embeddings = X_feature[test_idx,:,:]\n",
        "      train_labels = label_all[train_idx]\n",
        "      test_labels = label_all[test_idx]\n",
        "\n",
        "      N, H, W = train_embeddings.shape\n",
        "      train_embeddings = np.reshape(train_embeddings, [N,H,W,1])\n",
        "\n",
        "\n",
        "      N, H, W = test_embeddings.shape\n",
        "      test_embeddings = np.reshape(test_embeddings, [N,H,W,1])\n",
        "\n",
        "\n",
        "      model = initialize_model(num_classes)\n",
        "\n",
        "      model.compile(optimizer='adam',\n",
        "                    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                    metrics=['accuracy'])\n",
        "      \n",
        "      logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "      tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
        "\n",
        "      model.fit(x=train_embeddings, \n",
        "              y=train_labels, \n",
        "              epochs = 30,\n",
        "              batch_size = 6, \n",
        "              validation_data=(test_embeddings, test_labels), \n",
        "              callbacks=[tensorboard_callback], verbose = 1)\n",
        "      \n",
        "      predictions = model.predict(test_embeddings)\n",
        "\n",
        "      predicted_labals = np.argmax(predictions, axis = -1)\n",
        "\n",
        "      acc, auc, precision, recall, f1score = calculate_acc(predicted_labals, test_labels)\n",
        "\n",
        "      auc_all.append(auc)\n",
        "      acc_all.append(acc) \n",
        "      f1score_all.append(f1score)\n",
        "\n",
        "      print('fold ', fold_idx, ' acc ', acc, ' auc ', auc, ' precision ', precision, ' recall ', recall, ' f1score ', f1score) \n",
        "      \n",
        "      #### Predict_with_Logistic_regression\n",
        "      # extractor = tensorflow.keras.Model(inputs=model.inputs,\n",
        "      #                     outputs=[layer.output for layer in model.layers])\n",
        "\n",
        "      # features = extractor(X_feature)\n",
        "\n",
        "      # X_feature_new = features[5]\n",
        "\n",
        "      # X_feature_new = np.array(X_feature_new)\n",
        "\n",
        "      # train_embeddings = X_feature_new[train_idx,:]\n",
        "      # test_embeddings = X_feature_new[test_idx,:]\n",
        "      # train_labels = label_all[train_idx]\n",
        "      # test_labels = label_all[test_idx]\n",
        "\n",
        "      # cls = LogisticRegression(tol=0.001, max_iter = 2000)\n",
        "      # cls.fit(train_embeddings, train_labels)\n",
        "\n",
        "      # predicted_labals = cls.predict(test_embeddings)\n",
        "\n",
        "      # acc, auc = calculate_acc(predicted_labals, test_labels)\n",
        "      # print('fold ', fold_idx, ' acc ', acc, ' auc ', auc)\n",
        "\n",
        "      # auc_all.append(auc)\n",
        "      # acc_all.append(acc) \n",
        "\n",
        "      ## Predict with KNN\n",
        "      # extractor = tensorflow.keras.Model(inputs=model.inputs,\n",
        "      #                     outputs=[layer.output for layer in model.layers])\n",
        "\n",
        "      # features = extractor(X_feature)\n",
        "\n",
        "      # X_feature_new = features[7]\n",
        "      # X_feature_new = np.array(X_feature_new)\n",
        "\n",
        "      # predicted_labals = Knn_classifier(X_feature_new, label_all, train_idx, test_idx)\n",
        "      # acc, auc = calculate_acc(predicted_labals, test_labels)\n",
        "\n",
        "      # auc_all.append(auc)\n",
        "      # acc_all.append(acc)\n",
        "\n",
        "      # print('fold ', fold_idx, ' acc ', acc, ' auc ', auc) \n",
        "\n",
        "  acc_all = np.array(acc_all)\n",
        "  auc_all = np.array(auc_all)\n",
        "  f1score_all = np.array(f1score_all)\n",
        "\n",
        "  print('avg_acc = ',np.mean(acc_all), 'avg_auc = ',np.mean(auc_all))\n",
        "  print('acc_all = ', acc_all)   \n",
        "  print('auc_all = ', auc_all)   \n",
        "\n",
        "  return acc_all, auc_all, f1score_all\n",
        "\n",
        "total_acc = []\n",
        "total_auc = []\n",
        "total_f1score = []\n",
        "iter = 2\n",
        "for i in range(iter):\n",
        "  acc_all,auc_all, f1score_all = run_CNN()\n",
        "  total_acc.append(acc_all)\n",
        "  total_auc.append(auc_all)\n",
        "  total_f1score.append(f1score_all)\n",
        "\n",
        "print_results(total_acc, total_auc, total_f1score)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JjRxEpbUUgp3"
      },
      "outputs": [],
      "source": [
        "import tensorflow.keras\n",
        "\n",
        "extractor = tensorflow.keras.Model(inputs=model.inputs,\n",
        "                        outputs=[layer.output for layer in model.layers])\n",
        "\n",
        "features = extractor(X_feature)\n",
        "\n",
        "X = features[5]\n",
        "\n",
        "print(X.shape)\n",
        "\n",
        "\n",
        "visualize_with_tSNE(X, label_all)\n",
        "\n",
        "# X_feature_new = np.array(X)\n",
        "# print(X.shape)\n",
        "# auc_all = []\n",
        "# acc_all = []\n",
        "# # ##**** Logistic Regression **** #####\n",
        "# for fold_idx in range(5):\n",
        "#     train_idx, test_idx = separate_data_idx(label_all, fold_idx)\n",
        "#     train_embeddings = X_feature_new[train_idx,:]\n",
        "#     test_embeddings = X_feature_new[test_idx,:]\n",
        "#     train_labels = label_all[train_idx]\n",
        "#     test_labels = label_all[test_idx]\n",
        "\n",
        "\n",
        "\n",
        "#     cls = LogisticRegression(tol=0.001, max_iter = 2000)\n",
        "#     cls.fit(train_embeddings, train_labels)\n",
        "#     ACC = cls.score(test_embeddings, test_labels)\n",
        "\n",
        "#     predicted = cls.predict(test_embeddings)\n",
        "#     predicted_labels.append(predicted)\n",
        "\n",
        "#     acc, auc = calculate_acc(predicted, test_labels)\n",
        "\n",
        "#     print('fold ', fold_idx, ' acc ', ACC, ' auc ', auc) \n",
        "\n",
        "#     auc_all.append(auc)\n",
        "#     acc_all.append(acc)\n",
        "\n",
        "# acc_all = np.array(acc_all)\n",
        "# auc_all = np.array(auc_all)\n",
        "\n",
        "# print('mean_acc = ', np.mean(acc_all))\n",
        "\n",
        "# print('acc_all = ', acc_all)   \n",
        "\n",
        "# print('auc_all = ', auc_all) \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wXyjc3YSxJRO"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Fixing random state for reproducibility\n",
        "np.random.seed(19680801)\n",
        "\n",
        "\n",
        "# N = 10\n",
        "# r0 = 0.6\n",
        "# x = 0.9 * np.random.rand(N)\n",
        "# y = 0.9 * np.random.rand(N)\n",
        "# area = (20 * np.random.rand(N))**2  # 0 to 10 point radii\n",
        "# c = np.sqrt(area)\n",
        "# r = np.sqrt(x ** 2 + y ** 2)\n",
        "# area1 = np.ma.masked_where(r < r0, area)\n",
        "# area2 = np.ma.masked_where(r >= r0, area)\n",
        "# plt.scatter(x, y, s=area1, marker='^', c=c)\n",
        "# plt.scatter(x, y, s=area2, marker='o', c=c)\n",
        "# # Show the boundary between the regions:\n",
        "# theta = np.arange(0, np.pi / 2, 0.01)\n",
        "# plt.plot(r0 * np.cos(theta), r0 * np.sin(theta))\n",
        "\n",
        "# plt.show()\n",
        "\n",
        "\n",
        "x = np.random.rand(10,1)\n",
        "y = np.random.rand(10,1)\n",
        "plt.scatter(x, y, marker='^')\n",
        "plt.scatter(x+1, y+1, marker='o')\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "y45Q0HJanW8L",
        "lHd7IYzkkdsm",
        "lTizzhHQLLi0",
        "XWTNBL8QntFV",
        "F5CN2etPIXdR",
        "Mi52pXK6dbzW",
        "xg-e8cPJk2Fh",
        "GTWAzqOuq3jf",
        "qD2eFV1UnYx1",
        "zWn6veyhwIKV"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}